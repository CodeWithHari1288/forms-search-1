#!/usr/bin/env python3
import os, glob, json, ulid
import numpy as np
import pandas as pd
import redis
from dotenv import load_dotenv
from ollama import Client
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
from redis.commands.search.field import TextField, TagField, VectorField

load_dotenv()

# ---- Config ----
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
REDIS_USERNAME = os.getenv("REDIS_USERNAME") or None
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD") or None
REDIS_SSL = (os.getenv("REDIS_SSL", "false").lower() == "true")

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")

INDEX_NAME  = os.getenv("INDEX_NAME", "idx:forms")
KEY_PREFIX  = os.getenv("KEY_PREFIX", "form:")
CSV_DIR     = os.getenv("CSV_DIR", "./data")
FORM_ID_DEF = os.getenv("DEFAULT_FORM_ID", "A")
VECTOR_DIM  = int(os.getenv("VECTOR_DIM", "1024"))

# ---- Connect ----
r = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    username=REDIS_USERNAME,
    password=REDIS_PASSWORD,
    ssl=REDIS_SSL,
    decode_responses=False,
)
# sanity check
r.ping()

ollama = Client(host=OLLAMA_HOST)
ollama.show("bge-m3")  # verify availability

# ---- Embeddings ----
def embed_text(text: str) -> list[float]:
    """Return list[float] (length 1024) from Ollama bge-m3 embeddings."""
    resp = ollama.embeddings(model="bge-m3", prompt=text)
    vec = resp["embedding"]
    if len(vec) != VECTOR_DIM:
        raise ValueError(f"Expected {VECTOR_DIM}-D vector, got {len(vec)}")
    # keep JSON as array of floats (RedisJSON supports VECTOR over arrays)
    return vec

# ---- Index (ON JSON) ----
def ensure_index():
    try:
        r.ft(INDEX_NAME).info()
        print(f"[ok] Index {INDEX_NAME} exists.")
        return
    except Exception:
        pass

    schema = (
        # TEXT for BM25
        TextField("$.text",          as_name="text"),
        TextField("$.label",         as_name="label"),
        TextField("$.propertyname",  as_name="propertyname"),
        TextField("$.value",         as_name="value"),
        TextField("$.question_context", as_name="question_context"),
        TextField("$.repeatable",    as_name="repeatable"),
        TextField("$.pointer",       as_name="pointer"),

        # TAG filters
        TagField("$.form_id",    as_name="form_id"),
        TagField("$.section",    as_name="section"),
        TagField("$.subsection", as_name="subsection"),
        TagField("$.status",     as_name="status"),  # optional

        # VECTOR field over JSON array
        VectorField(
            "$.vec",
            "HNSW",
            {
                "TYPE":"FLOAT32",
                "DIM":VECTOR_DIM,
                "DISTANCE_METRIC":"COSINE",
                "M":16,
                "EF_CONSTRUCTION":200,
                "EF_RUNTIME":64
            },
            as_name="vec"
        ),
    )
    definition = IndexDefinition(prefix=[KEY_PREFIX], index_type=IndexType.JSON)
    r.ft(INDEX_NAME).create_index(schema, definition=definition)
    print(f"[ok] Created index {INDEX_NAME} (ON JSON).")

# ---- CSV -> JSON doc model ----
def row_to_doc(row: pd.Series, form_id: str) -> dict:
    # CSV columns (as you provided):
    # Prop, question_id, question_label, section, sub_section,
    # question_context, repeatable, value, json_pointer
    label = str(row.get("question_label", "") or "")
    prop  = str(row.get("Prop", "") or "")
    sec   = str(row.get("section", "") or "")
    sub   = str(row.get("sub_section", "") or "")
    ctx   = "" if pd.isna(row.get("question_context")) else str(row.get("question_context"))
    rep   = "" if pd.isna(row.get("repeatable")) else str(row.get("repeatable"))
    val   = "" if pd.isna(row.get("value")) else str(row.get("value"))
    ptr   = str(row.get("json_pointer", "") or "")
    qid   = str(row.get("question_id", "") or "")

    # Human text for BM25 + embedding
    text = (
        f"Section: {sec}. Subsection: {sub}. "
        f"Field: {label}. Property: {prop}. "
        f"Context: {ctx}. Repeatable: {rep}."
    ).strip()

    doc = {
        "form_id": form_id,
        "section": sec,
        "subsection": sub,
        "label": label,
        "propertyname": prop,
        "value": val,
        "pointer": ptr,
        "text": text,
        "question_context": ctx,
        "repeatable": rep,
        "question_id": qid,
        # vec set later
    }
    return doc

def ingest_csv(file_path: str, form_id: str):
    df = pd.read_csv(file_path)
    required = [
        "Prop","question_id","question_label","section","sub_section",
        "question_context","repeatable","value","json_pointer"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"{os.path.basename(file_path)} missing columns: {missing}")

    total = 0
    for _, row in df.iterrows():
        doc = row_to_doc(row, form_id=form_id)
        doc["vec"] = embed_text(doc["text"])
        key = f"{KEY_PREFIX}{form_id}:row:{ulid.new().str}"
        r.json().set(key, "$", doc)
        total += 1
    return total

def ingest_folder(csv_dir: str):
    files = sorted(glob.glob(os.path.join(csv_dir, "*.csv")))
    if not files:
        print(f"[warn] no CSV files in {csv_dir}")
        return

    grand = 0
    for f in files:
        form_id = FORM_ID_DEF  # or derive from filename if needed
        print(f"[info] ingesting {os.path.basename(f)} (form_id={form_id})")
        count = ingest_csv(f, form_id)
        print(f"[ok] {count} rows")
        grand += count
    print(f"[done] total rows ingested: {grand}")

if __name__ == "__main__":
    ensure_index()
    ingest_folder(CSV_DIR)