#!/usr/bin/env python3
import os, glob, json
import numpy as np
import pandas as pd
import redis
from ollama import Client
from dotenv import load_dotenv
from redis.commands.search.field import TextField, VectorField, TagField
from redis.commands.search.indexDefinition import IndexDefinition
from redis.commands.search import IndexType

load_dotenv()

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
INDEX_NAME  = os.getenv("INDEX_NAME", "json_field_index")
PREFIX      = os.getenv("INDEX_PREFIX", "json_field:")
CSV_DIR     = os.getenv("CSV_DIR", "./data")
VECTOR_DIM  = int(os.getenv("VECTOR_DIM", "1024"))

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=False)
r.ping()
ollama_client = Client(host=OLLAMA_HOST)
ollama_client.show("bge-m3")

def embed(text: str) -> bytes:
    resp = ollama_client.embeddings(model="bge-m3", prompt=text)
    return np.array(resp["embedding"], dtype=np.float32).tobytes()

def norm_tag(v: str) -> str:
    if v is None: return ""
    return str(v).strip().replace(" ", r"\ ")

def infer_shape(value: str):
    try:
        obj = json.loads(value)
        if isinstance(obj, list):  return ("list", len(obj))
        if isinstance(obj, dict):  return ("map",  len(obj))
    except Exception:
        pass
    return ("scalar", 1)

def build_text_chunk(row):
    sec = str(row.get("section","")); sub = str(row.get("subsection",""))
    val = "" if pd.isna(row.get("json_value")) else str(row.get("json_value"))
    return f"question: {row.get('question_label','')}\nproperty: {row.get('json_property','')}\nvalue: {val}\njson_pointer: {row.get('json_pointer','')}\nsection: {sec} / subsection: {sub}"

def build_text_boost(row):
    def toks(s: str):
        return [t for t in ''.join(ch if ch.isalnum() else ' ' for ch in s).lower().split() if len(t)>2]
    label = str(row.get("question_label","")); prop = str(row.get("json_property",""))
    val = "" if pd.isna(row.get("json_value")) else str(row.get("json_value"))
    val_short = val[:160]
    tokens = toks(label)+toks(prop)+toks(val_short)
    dup = tokens + tokens
    low = (label+" "+prop+" "+val_short).lower()
    hints=[]; 
    if "addr" in low or "address" in low: hints += ["address"]*4
    if "id" in low: hints += ["id"]*3
    if "pointer" in low or "path" in low: hints += ["pointer"]*3
    if "name" in low: hints += ["name"]*2
    if "date" in low: hints += ["date"]*2
    return " ".join(dup+hints)

def ensure_index():
    try:
        r.ft(INDEX_NAME).info(); print(f"[ok] Index '{INDEX_NAME}' exists."); return
    except Exception: pass
    schema = (
        TextField("text_chunk"),
        TextField("text_boost"),
        TextField("json_pointer"),
        TextField("json_property"),
        TextField("json_value"),
        TextField("question_id"),
        TextField("question_label"),
        TagField("section"),
        TagField("subsection"),
        TextField("shape"),
        TextField("shape_size"),
        VectorField("vector","HNSW",{"TYPE":"FLOAT32","DIM":VECTOR_DIM,"DISTANCE_METRIC":"COSINE","M":16,"EF_CONSTRUCTION":200,"EF_RUNTIME":64}),
    )
    definition = IndexDefinition(prefix=[PREFIX], index_type=IndexType.HASH)
    r.ft(INDEX_NAME).create_index(schema, definition=definition)
    print(f"[ok] Created index '{INDEX_NAME}'.")

def read_and_normalize_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    rename_map = {
        "Question id":"question_id","Question label":"question_label","Prop":"json_property","Value":"json_value",
        "Section":"section","Subsection":"subsection","Json pointer":"json_pointer","Json path":"json_pointer"
    }
    for k,v in rename_map.items():
        if k in df.columns: df.rename(columns={k:v}, inplace=True)
    required = ["question_id","question_label","json_property","json_value","section","subsection","json_pointer"]
    missing = [c for c in required if c not in df.columns]
    if missing: raise ValueError(f"{os.path.basename(path)} missing columns: {missing}")
    return df

def ingest_folder(folder: str):
    files = sorted(glob.glob(os.path.join(folder, "*.csv")))
    if not files: print(f"[warn] No CSV files in {folder}"); return
    total=0
    for fp in files:
        print(f"[info] Ingesting {os.path.basename(fp)}")
        df = read_and_normalize_csv(fp); src=os.path.basename(fp)
        for i,row in df.iterrows():
            qid=str(row["question_id"]); qlbl=str(row["question_label"]); prop=str(row["json_property"])
            val="" if pd.isna(row["json_value"]) else str(row["json_value"]); ptr=str(row["json_pointer"])
            sec=norm_tag(row["section"]); sub=norm_tag(row["subsection"])
            text_chunk=build_text_chunk(row); text_boost=build_text_boost(row); vec=embed(f"passage: {text_chunk}")
            shape,size = infer_shape(val)
            key=f"{PREFIX}{src}:{qid or i}"
            r.hset(key, mapping={
                "question_id": qid, "question_label": qlbl, "json_property": prop, "json_value": val,
                "json_pointer": ptr, "section": sec, "subsection": sub, "shape": shape, "shape_size": str(size),
                "text_chunk": text_chunk, "text_boost": text_boost, "vector": vec,
            })
            total+=1
    print(f"[ok] Ingested {total} rows from {len(files)} files.")

if __name__ == "__main__":
    ensure_index(); ingest_folder(CSV_DIR)








#!/usr/bin/env python3
import os
import numpy as np
import redis
from ollama import Client
from dotenv import load_dotenv
from redis.commands.search.query import Query

load_dotenv()

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
INDEX_NAME  = os.getenv("INDEX_NAME", "json_field_index")
PREFIX      = os.getenv("INDEX_PREFIX", "json_field:")

DEFAULT_TEXT_FIELDS = ["text_boost","text_chunk","question_label","json_property","json_value"]

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=False)
r.ping()
ollama_client = Client(host=OLLAMA_HOST)
ollama_client.show("bge-m3")

def _b(x): return x.decode() if isinstance(x,(bytes,bytearray)) else x
def _escape_tag(s): return "" if s is None else str(s).strip().replace(" ", r"\ ")
def _escape_phrase(s): return str(s).replace('"', r'\"')

def get_vec(text: str) -> bytes:
    resp = ollama_client.embeddings(model="bge-m3", prompt=text)
    return np.array(resp["embedding"], dtype=np.float32).tobytes()

def _project_fields():
    return ["vector_score","question_id","question_label","json_property","json_value","json_pointer","section","subsection","shape","shape_size","text_chunk"]

def dense_search(query_text: str, k=5, section=None, subsection=None, equals_filters=None, require_shape=None, return_fields=None):
    vec = get_vec(f"query: {query_text}")
    filters=[]
    if section: filters.append(f"@section:{{{_escape_tag(section)}}}")
    if subsection: filters.append(f"@subsection:{{{_escape_tag(subsection)}}}")
    if require_shape in {"list","map","scalar"}: filters.append(f"@shape:{require_shape}")
    if equals_filters:
        for fld,val in (equals_filters or {}).items():
            filters.append(f'@{fld}:"{_escape_phrase(val)}"')
    prefix = " ".join(filters) if filters else "*"
    q = Query(f"{prefix}=>[KNN {k} @vector $vec AS vector_score]")\
        .return_fields(*(return_fields or _project_fields()))\
        .sort_by("vector_score", asc=True)\
        .dialect(4)\
        .with_params({"vec": vec})
    res = r.ft(INDEX_NAME).search(q)
    out=[]
    for d in res.docs:
        out.append({
            "distance": float(d.vector_score),
            "question_id": _b(getattr(d,"question_id","")),
            "question_label": _b(getattr(d,"question_label","")),
            "json_property": _b(getattr(d,"json_property","")),
            "json_value": _b(getattr(d,"json_value","")),
            "json_pointer": _b(getattr(d,"json_pointer","")),
            "section": _b(getattr(d,"section","")),
            "subsection": _b(getattr(d,"subsection","")),
            "shape": _b(getattr(d,"shape","")),
            "shape_size": _b(getattr(d,"shape_size","")),
            "chunk": _b(getattr(d,"text_chunk",""))[:240],
        })
    return out
