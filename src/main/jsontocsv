# ingest_csv_chroma_folder.py
# Purpose: Loop over all CSV files in a folder, embed each with BGE-M3 (dense), and upsert to ChromaDB.
# Notes:
# - Each CSV must have a header row.
# - Uses your "card text" strategy and payload metadata (form_id, pointer, etc.)
# - Uses dense-only embeddings; sparse/lexical hybrid handled at query time.

import os, csv, argparse
from typing import Dict, List, Any
import chromadb
from FlagEmbedding import BGEM3FlagModel

# ---------- Helpers ----------
def rows_from_csv(path: str) -> List[Dict[str, str]]:
    with open(path, "r", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def build_text(row: Dict[str, str]) -> str:
    label = (row.get("label") or "").strip()
    aliases = (row.get("aliases") or "").strip()
    section = (row.get("section") or "").strip()
    subsection = (row.get("subsection") or "").strip()
    enum_labels = (row.get("enum_labels") or "").strip()
    datatype = (row.get("datatype") or "").strip()
    parts = [
        f"Section: {section}" if section else "",
        f"Subsection: {subsection}" if subsection else "",
        f"Field: {label}" if label else "",
        f"Aliases: {aliases}" if aliases else "",
        f"Type: {datatype}" if datatype else "",
        f"Enum: {enum_labels}" if enum_labels else "",
    ]
    return ". ".join([p for p in parts if p])

def payload_from_row(row: Dict[str, str]) -> Dict[str, Any]:
    return {
        "form_id": row.get("form_id", ""),
        "section": row.get("section", ""),
        "subsection": row.get("subsection", ""),
        "pointer": row.get("pointer", ""),
        "pattern_path": row.get("pattern_path", ""),
        "json_property_name": row.get("json_property_name", ""),
        "label": row.get("label", ""),
        "aliases": row.get("aliases", ""),
        "datatype": row.get("datatype", ""),
        "unit": row.get("unit", ""),
        "enum_labels": row.get("enum_labels", ""),
        "status": row.get("status", ""),
        "version_id": row.get("version_id", ""),
        "version_ts": row.get("version_ts", ""),
        "row_id": row.get("row_id", ""),
        "is_field_card": True,
        "is_section_card": False,
        "lang": row.get("lang", ""),
        "source_file": os.path.basename(row.get("_source", "")),  # added for traceability
    }

def find_csv_files(folder: str) -> List[str]:
    files = []
    for f in os.listdir(folder):
        if f.lower().endswith(".csv"):
            files.append(os.path.join(folder, f))
    return files

# ---------- Main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--folder", required=True, help="Folder path containing CSV files")
    ap.add_argument("--collection", default=os.environ.get("COLLECTION", "forms_demo"))
    ap.add_argument("--persist_dir", default=os.environ.get("CHROMA_DIR", "./chroma_db"))
    ap.add_argument("--batch", type=int, default=256)
    args = ap.parse_args()

    csv_files = find_csv_files(args.folder)
    if not csv_files:
        print(f"No CSV files found in folder: {args.folder}")
        return

    print(f"Found {len(csv_files)} CSV file(s) to process in {args.folder}")
    client = chromadb.PersistentClient(path=args.persist_dir)
    coll = client.get_or_create_collection(name=args.collection)
    model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

    total = 0
    for csv_path in csv_files:
        print(f"\nüìÑ Processing: {csv_path}")
        rows = rows_from_csv(csv_path)
        if not rows:
            print(f"‚ö†Ô∏è  Skipped empty CSV: {csv_path}")
            continue

        # Attach source info for traceability
        for r in rows:
            r["_source"] = csv_path

        texts = [build_text(r) for r in rows]
        payloads = [payload_from_row(r) for r in rows]
        ids = [f"{r.get('form_id','')}:{r.get('pointer','')}:{i}:{os.path.basename(csv_path)}" for i, r in enumerate(rows)]

        B = max(1, args.batch)
        for start in range(0, len(rows), B):
            end = min(start + B, len(rows))
            batch_texts = texts[start:end]
            out = model.encode(batch_texts, return_dense=True, return_sparse=False)
            dense = out["dense_vecs"]

            coll.upsert(
                ids=ids[start:end],
                documents=batch_texts,
                metadatas=payloads[start:end],
                embeddings=dense,
            )
            print(f"‚úÖ Upserted {end}/{len(rows)} rows from {os.path.basename(csv_path)}")

        total += len(rows)

    print(f"\nüéØ Done. Total records embedded: {total}")
    print(f"‚úÖ All data persisted to collection '{args.collection}' in {args.persist_dir}")

if __name__ == "__main__":
    main()