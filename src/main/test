from typing import Any, Dict, List, Tuple, Iterable, Union
from collections import defaultdict
import re

Json = Union[Dict[str, Any], List[Any], str, int, float, bool, None]

# ---- helpers ----
_WORDS = re.compile(r"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\d+")

def _split_words(s: str) -> List[str]:
    return [m.group(0) for m in _WORDS.finditer((s or "").replace("_", " "))] or [s or ""]

def _title(label: str) -> str:
    return " ".join(w[:1].upper() + w[1:] for w in _split_words(label) if w)

def _dedupe_adjacent(segments: Iterable[str]) -> List[str]:
    out, last = [], None
    for seg in segments:
        if last is None or seg.lower() != last.lower():
            out.append(seg); last = seg
    return out

def _node_label(path: List[str]) -> str:
    return _title(path[-1]) if path else "Root"

# ---- core: JSON → only "has" statements, grouped per entity ----
def json_to_has_sections(
    data: Json,
    *,
    root_name: str = "Root",
    ignore_keys: Iterable[str] = ("_id", "id", "@type", "type"),
    skip_empty: bool = True,
) -> List[Dict[str, str]]:
    """
    Returns a list of sections:
      { "id": "<stable-id>", "entity": "<EntityName>", "text": "<lines joined by \\n>" }
    Lines look like: "<Entity> has <Field> = <Value>"
    """
    ignore = {k.lower() for k in ignore_keys}
    # entity -> set(lines)
    by_entity: Dict[str, List[str]] = defaultdict(list)

    def walk(node: Json, path: List[str]):
        if isinstance(node, dict):
            for k, v in node.items():
                if isinstance(k, str) and k.lower() in ignore:
                    walk(v, path)  # skip naming this key but keep traversing
                    continue
                walk(v, _dedupe_adjacent(path + [k]))
        elif isinstance(node, list):
            # no indices; traverse items at same path
            for item in node:
                walk(item, path)
        else:
            # leaf → "<Entity> has <Field> = <Value>"
            entity = _node_label(path[:-1]) if len(path) > 1 else _title(root_name)
            field  = _title(path[-1]) if path else _title("Value")
            val = "" if node is None else str(node)
            if skip_empty and not str(val).strip():
                return
            by_entity[entity].append(f"{entity} has {field} = {val}")

    walk(data, [root_name])

    # build sections: sort lines & dedupe case-insensitively
    sections: List[Dict[str, str]] = []
    for entity in sorted(by_entity.keys(), key=str.lower):
        lines = by_entity[entity]
        # dedupe while preserving order
        seen, uniq = set(), []
        for s in lines:
            k = s.lower()
            if k not in seen:
                uniq.append(s); seen.add(k)
        uniq.sort(key=str.lower)  # stable, tidy order
        sections.append({
            "id": f"{entity.replace(' ', '_').lower()}",
            "entity": entity,
            "text": "\n".join(uniq),
        })
    return sections

# ---- demo ----
if __name__ == "__main__":
    sample = {
        "customer": {
            "customer": {
                "name": "Hari",
                "city": "Hyderabad",
                "address": {"city": "Hyderabad", "pinCode": "500001"}
            },
            "orders": [
                {"order": {"id": "123", "amount": 500}},
                {"order": {"id": "456", "amount": 900}}
            ]
        }
    }
    secs = json_to_has_sections(sample, root_name="Customer")
    for s in secs:
        print(f"\n=== [{s['id']}] {s['entity']} ===")
        print(s["text"])






from sentence_transformers import SentenceTransformer
import torch

model = SentenceTransformer(
    "BAAI/bge-m3",
    trust_remote_code=True,
    device="cuda" if torch.cuda.is_available() else "cpu",
)

def encode_docs(texts, batch_size=64):
    return model.encode(texts, normalize_embeddings=True, batch_size=batch_size, show_progress_bar=True)

# build sections
sections = json_to_has_sections(sample, root_name="Customer")
doc_ids   = [s["id"] for s in sections]
doc_texts = [s["text"] for s in sections]
doc_embs  = encode_docs(doc_texts)   # np.ndarray [N, D]