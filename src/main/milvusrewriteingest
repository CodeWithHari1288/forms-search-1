#!/usr/bin/env python3
"""
Ingest a schema-only collection for rewrite/query understanding in LangGraph.

Collection: form_schema_index

Fields:
  - id (INT64, auto_id primary key)
  - Question_label   (VARCHAR)
  - Prop             (VARCHAR)
  - Section          (VARCHAR)
  - Subsection       (VARCHAR)
  - dense            (FLOAT_VECTOR, 1024)  <-- from BGE-M3 via Ollama

No Value / Json_pointer / Question_context here.
This is ONLY for mapping user NLP â†’ schema fields in your rewrite node.
"""

import os
import glob
from typing import List, Dict, Any

import pandas as pd
import requests
from pymilvus import (
    MilvusClient,
    DataType,
    FieldSchema,
    CollectionSchema,
)

# ----------------------------
# CONFIG
# ----------------------------
MILVUS_URI = "http://localhost:19530"   # Milvus standalone
CSV_DIR = "./csvs"                      # folder containing your CSV files
SCHEMA_COLLECTION = "form_schema_index"

# BGE-M3 dense dimension
DENSE_DIM = 1024

# Batch insert size
BATCH_SIZE = 100

# Ollama config
OLLAMA_URL = "http://localhost:11434/api/embeddings"
OLLAMA_MODEL = "bge-m3"

# CSV headers we expect (we only use some of them)
REQUIRED_COLS = [
    "Question_label",
    "Prop",
    "Value",
    "Json_pointer",
    "Question_context",
    "Section",
    "Subsection",
]


# ----------------------------
# CONNECT TO MILVUS
# ----------------------------
client = MilvusClient(uri=MILVUS_URI)


# ----------------------------
# COLLECTION DEFINITION
# ----------------------------
def ensure_schema_collection():
    if client.has_collection(SCHEMA_COLLECTION):
        print(f"âœ… Collection '{SCHEMA_COLLECTION}' already exists.")
        return

    fields = [
        FieldSchema(
            name="id",
            dtype=DataType.INT64,
            is_primary=True,
            auto_id=True,
        ),
        FieldSchema(
            name="Question_label",
            dtype=DataType.VARCHAR,
            max_length=512,
        ),
        FieldSchema(
            name="Prop",
            dtype=DataType.VARCHAR,
            max_length=256,
        ),
        FieldSchema(
            name="Section",
            dtype=DataType.VARCHAR,
            max_length=256,
        ),
        FieldSchema(
            name="Subsection",
            dtype=DataType.VARCHAR,
            max_length=256,
        ),
        FieldSchema(
            name="dense",
            dtype=DataType.FLOAT_VECTOR,
            dim=DENSE_DIM,
        ),
    ]

    schema = CollectionSchema(
        fields=fields,
        description="Schema-only collection (dense BGE-M3) for query rewrite",
    )

    client.create_collection(
        collection_name=SCHEMA_COLLECTION,
        schema=schema,
    )
    print(f"ðŸ†• Created collection '{SCHEMA_COLLECTION}'.")

    # Create index on dense vector
    print("ðŸ“¦ Creating HNSW index on 'dense' ...")
    client.create_index(
        collection_name=SCHEMA_COLLECTION,
        field_name="dense",
        index_params={
            "index_type": "HNSW",
            "metric_type": "COSINE",
            "params": {"M": 16, "efConstruction": 64},
        },
    )

    client.load_collection(SCHEMA_COLLECTION)
    print("âœ… Index created and collection loaded.")


# ----------------------------
# OLLAMA EMBEDDINGS (BGE-M3)
# ----------------------------
def get_bgem3_embedding(text: str):
    """
    Call Ollama /api/embeddings for BGE-M3 and return a 1024-dim vector.
    """
    resp = requests.post(
        OLLAMA_URL,
        json={
            "model": OLLAMA_MODEL,
            "prompt": text,
        },
        timeout=120,
    )
    resp.raise_for_status()
    data = resp.json()

    # Ollama returns: {"embedding": [...], ...}
    emb = data.get("embedding")
    if emb is None:
        raise RuntimeError(f"No 'embedding' field in Ollama response: {data}")

    if len(emb) != DENSE_DIM:
        print(f"âš ï¸ Warning: expected dim {DENSE_DIM}, got {len(emb)}")

    return emb


def embed_schema_text(label: str, prop: str, section: str, subsection: str):
    """
    Build the schema text to embed.

    You can adjust this prompt style anytime:
      e.g., include more hints like "field label", "property name", etc.
    """
    parts = [label, f"({prop})"]
    if section:
        parts.append(f"section: {section}")
    if subsection:
        parts.append(f"subsection: {subsection}")
    text = " - ".join(parts)

    return get_bgem3_embedding(text)


# ----------------------------
# INGESTION
# ----------------------------
def ingest_csv_folder(folder: str):
    csv_files = glob.glob(os.path.join(folder, "*.csv"))
    if not csv_files:
        print(f"âŒ No CSV files found in {folder}")
        return

    ensure_schema_collection()

    for path in csv_files:
        print(f"ðŸ“„ Reading {os.path.basename(path)} ...")
        df = pd.read_csv(path)

        missing = [c for c in REQUIRED_COLS if c not in df.columns]
        if missing:
            print(f"âš ï¸ Missing columns {missing} in {path}, skipping.")
            continue

        ingest_df(df)


def ingest_df(df: pd.DataFrame):
    batch: List[Dict[str, Any]] = []

    for _, row in df.iterrows():
        q_label = str(row["Question_label"])
        prop = str(row["Prop"])
        section = "" if pd.isna(row["Section"]) else str(row["Section"])
        subsection = "" if pd.isna(row["Subsection"]) else str(row["Subsection"])

        dense_vec = embed_schema_text(q_label, prop, section, subsection)

        record = {
            "Question_label": q_label,
            "Prop": prop,
            "Section": section,
            "Subsection": subsection,
            "dense": dense_vec,
        }
        batch.append(record)

        if len(batch) >= BATCH_SIZE:
            flush_batch(batch)

    if batch:
        flush_batch(batch)


def flush_batch(batch: List[Dict[str, Any]]):
    print(f"â¬†ï¸ Inserting batch of {len(batch)} rows into '{SCHEMA_COLLECTION}' ...")
    client.insert(collection_name=SCHEMA_COLLECTION, data=batch)
    print("âœ… Batch inserted.")


# ----------------------------
# MAIN
# ----------------------------
if __name__ == "__main__":
    ingest_csv_folder(CSV_DIR)
    print("ðŸŽ‰ Finished ingesting schema collection with dense BGE-M3 embeddings.")











# rewrite_query_bgem3.py

from typing import List, Dict, Any
from typing_extensions import TypedDict
import os
import json

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from pymilvus import MilvusClient
from FlagEmbedding import BGEM3FlagModel

# -------------------------
# CONFIG
# -------------------------
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
SCHEMA_COLLECTION = os.getenv("SCHEMA_COLLECTION", "form_schema_index")

DENSE_DIM = 1024
TOP_K_SCHEMA = 8   # how many schema fields to show LLM

# -------------------------
# LANGGRAPH STATE
# -------------------------
class RewriteState(TypedDict):
    # Input
    user_query: str

    # Outputs
    schema_hits: List[Dict[str, Any]]
    structured_query: Dict[str, Any]


# -------------------------
# MILVUS CLIENT
# -------------------------
client = MilvusClient(uri=MILVUS_URI)


# -------------------------
# BGEM3 FLAG MODEL
# -------------------------
# Same model you used in ingest (BAAI/bge-m3)
print("Loading BGE-M3 FlagEmbedding model for rewrite...")
embed_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)


def get_bgem3_embedding(text: str) -> List[float]:
    """
    Use BGEM3FlagModel to get a dense 1024-d embedding.
    We only use dense here (no sparse) for schema rewrite collection.
    """
    out = embed_model.encode(
        text,
        return_dense=True,
        return_sparse=False,
        normalize_embeddings=True,
    )
    vec = out["dense_vecs"][0]
    if len(vec) != DENSE_DIM:
        print(f"âš ï¸ BGEM3 dim mismatch: expected {DENSE_DIM}, got {len(vec)}")
    return vec


def search_schema_in_milvus(query: str, k: int = TOP_K_SCHEMA) -> List[Dict[str, Any]]:
    """
    Vector search against form_schema_index using BGE-M3 embedding.
    Returns a list[dict] with label/prop/section/subsection + score.
    """
    q_vec = get_bgem3_embedding(query)

    res = client.search(
        collection_name=SCHEMA_COLLECTION,
        data=[q_vec],
        anns_field="dense",
        limit=k,
        output_fields=["Question_label", "Prop", "Section", "Subsection"],
        search_params={
            "metric_type": "COSINE",
            "params": {},
        },
    )

    hits: List[Dict[str, Any]] = []
    if not res:
        return hits

    for hit in res[0]:
        entity = hit.get("entity", {})
        hits.append(
            {
                "id": hit.get("id"),
                "score": hit.get("distance"),
                "Question_label": entity.get("Question_label", ""),
                "Prop": entity.get("Prop", ""),
                "Section": entity.get("Section", ""),
                "Subsection": entity.get("Subsection", ""),
            }
        )
    return hits


# -------------------------
# LLM FOR REWRITE
# -------------------------
# Swap this to your own LLM if needed (OpenAI, local, etc.)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
)


def build_rewrite_prompt(
    user_query: str,
    schema_hits: List[Dict[str, Any]],
) -> List[Any]:
    """
    Build messages to instruct the LLM to produce a structured JSON query.
    """
    lines = []
    for i, f in enumerate(schema_hits, start=1):
        lines.append(
            f"{i}. label={f['Question_label']} | prop={f['Prop']} "
            f"| section={f['Section']} | subsection={f['Subsection']} "
            f"(score={round(f['score'], 4) if f['score'] is not None else 'NA'})"
        )
    fields_text = "\n".join(lines) if lines else "(no schema hits)"

    system = SystemMessage(
        content=(
            "You are a query rewriting assistant for a forms database.\n"
            "You are given:\n"
            "1) A natural language user query.\n"
            "2) A list of candidate fields (label, prop, section, subsection).\n\n"
            "Your job:\n"
            "- Choose which fields are relevant to the user query.\n"
            "- Produce a STRICT JSON object (no extra text) describing "
            "a structured query the backend can use.\n\n"
            "Output JSON format:\n"
            "{\n"
            '  "filters": [\n'
            '    {"prop": string, "section": string, "subsection": string},\n'
            "    ...\n"
            "  ],\n"
            '  "original_query": string,\n'
            '  "rewrite": string\n'
            "}\n\n"
            "- If a field is relevant but section/subsection is empty, set them to \"\".\n"
            "- If nothing matches, return filters: [].\n"
            "- Do NOT include comments or explanation. Only raw JSON."
        )
    )

    human = HumanMessage(
        content=(
            f"User query:\n{user_query}\n\n"
            f"Candidate fields:\n{fields_text}\n\n"
            "Now return ONLY the JSON object in the required format."
        )
    )

    return [system, human]


# -------------------------
# LANGGRAPH NODE
# -------------------------
def rewrite_query_node(state: RewriteState) -> Dict[str, Any]:
    """
    Node:
      input : state['user_query']
      output: state['schema_hits'], state['structured_query']
    """
    user_query = state["user_query"]

    # 1) Vector search on schema collection using bgem3
    schema_hits = search_schema_in_milvus(user_query, k=TOP_K_SCHEMA)

    # 2) LLM rewrite â†’ structured query JSON
    messages = build_rewrite_prompt(user_query, schema_hits)
    raw = llm.invoke(messages)
    txt = raw.content.strip()

    try:
        structured = json.loads(txt)
    except json.JSONDecodeError:
        # crude fallback if the model adds extra text
        start = txt.find("{")
        end = txt.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                structured = json.loads(txt[start : end + 1])
            except json.JSONDecodeError:
                structured = {"error": "LLM did not return valid JSON", "raw": txt}
        else:
            structured = {"error": "LLM did not return JSON", "raw": txt}

    return {
        "schema_hits": schema_hits,
        "structured_query": structured,
    }


# -------------------------
# BUILD GRAPH
# -------------------------
builder = StateGraph(RewriteState)

builder.add_node("rewrite_query", rewrite_query_node)
builder.add_edge(START, "rewrite_query")
builder.add_edge("rewrite_query", END)

rewrite_graph = builder.compile()


if __name__ == "__main__":
    # quick test
    test_state = {"user_query": "Get all PAN and GSTIN fields for billing address"}
    result = rewrite_graph.invoke(test_state)

    from pprint import pprint
    print("\n=== Schema hits ===")
    pprint(result["schema_hits"])
    print("\n=== Structured query ===")
    pprint(result["structured_query"])












# rewrite_query_bgem3_only.py
"""
LangGraph node for query rewrite using only:
- BGEM3FlagModel (BAAI/bge-m3) for embeddings
- Milvus for schema search

No GPT / OpenAI / LLM.

Collection used: form_schema_index
Fields expected in that collection:
  - Question_label (VARCHAR)
  - Prop           (VARCHAR)
  - Section        (VARCHAR)
  - Subsection     (VARCHAR)
  - dense          (FLOAT_VECTOR, 1024)
"""

from typing import List, Dict, Any
from typing_extensions import TypedDict
import os

from langgraph.graph import StateGraph, START, END

from pymilvus import MilvusClient
from FlagEmbedding import BGEM3FlagModel


# -------------------------
# CONFIG
# -------------------------
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
SCHEMA_COLLECTION = os.getenv("SCHEMA_COLLECTION", "form_schema_index")

DENSE_DIM = 1024
TOP_K_SCHEMA = 8          # how many schema fields to consider
SCORE_THRESHOLD = 0.0     # COSINE similarity; 0.0 keeps all top-k


# -------------------------
# LANGGRAPH STATE
# -------------------------
class RewriteState(TypedDict):
    # Input
    user_query: str

    # Outputs (filled by node)
    schema_hits: List[Dict[str, Any]]
    structured_query: Dict[str, Any]


# -------------------------
# MILVUS CLIENT
# -------------------------
client = MilvusClient(uri=MILVUS_URI)


# -------------------------
# BGEM3 EMBEDDINGS
# -------------------------
print("Loading BGE-M3 FlagEmbedding model for rewrite...")
embed_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)


def get_bgem3_embedding(text: str) -> List[float]:
    """
    Use BGEM3FlagModel to get a dense 1024-d embedding.
    We only use dense for schema rewrite collection.
    """
    out = embed_model.encode(
        text,
        return_dense=True,
        return_sparse=False,
        normalize_embeddings=True,
    )
    vec = out["dense_vecs"][0]
    if len(vec) != DENSE_DIM:
        print(f"âš ï¸ BGEM3 dim mismatch: expected {DENSE_DIM}, got {len(vec)}")
    return vec


# -------------------------
# SCHEMA SEARCH
# -------------------------
def search_schema_in_milvus(query: str, k: int = TOP_K_SCHEMA) -> List[Dict[str, Any]]:
    """
    Vector search against form_schema_index using BGE-M3 embedding.
    Returns a list[dict] with label/prop/section/subsection + score.
    """
    q_vec = get_bgem3_embedding(query)

    res = client.search(
        collection_name=SCHEMA_COLLECTION,
        data=[q_vec],
        anns_field="dense",
        limit=k,
        output_fields=["Question_label", "Prop", "Section", "Subsection"],
        search_params={
            "metric_type": "COSINE",
            "params": {},
        },
    )

    hits: List[Dict[str, Any]] = []
    if not res:
        return hits

    for hit in res[0]:
        entity = hit.get("entity", {})
        score = hit.get("distance")
        # For COSINE in Milvus, "distance" is similarity when metric_type=COSINE
        hits.append(
            {
                "id": hit.get("id"),
                "score": score,
                "Question_label": entity.get("Question_label", ""),
                "Prop": entity.get("Prop", ""),
                "Section": entity.get("Section", ""),
                "Subsection": entity.get("Subsection", ""),
            }
        )

    return hits


# -------------------------
# SIMPLE RULE-BASED "REWRITE"
# -------------------------
def build_structured_query(
    user_query: str,
    schema_hits: List[Dict[str, Any]],
    score_threshold: float = SCORE_THRESHOLD,
) -> Dict[str, Any]:
    """
    Convert top schema hits into a simple structured JSON filter.

    No LLM here: we just pick the best matches from Milvus.
    """
    filters: List[Dict[str, str]] = []

    for h in schema_hits:
        score = h.get("score") or 0.0
        # If you want to drop weak matches, you can set SCORE_THRESHOLD > 0
        if score < score_threshold:
            continue

        filters.append(
            {
                "prop": h.get("Prop", ""),
                "section": h.get("Section", "") or "",
                "subsection": h.get("Subsection", "") or "",
            }
        )

    # very basic "rewrite" string without LLM
    if filters:
        prop_list = [f["prop"] for f in filters if f["prop"]]
        rewrite_str = (
            f"Use fields: {', '.join(prop_list)} for the query: {user_query}"
            if prop_list
            else user_query
        )
    else:
        rewrite_str = user_query

    return {
        "filters": filters,
        "original_query": user_query,
        "rewrite": rewrite_str,
    }


# -------------------------
# LANGGRAPH NODE
# -------------------------
def rewrite_query_node(state: RewriteState) -> Dict[str, Any]:
    """
    Node:
      input : state['user_query']
      output: state['schema_hits'], state['structured_query']

    Uses only:
      - BGEM3 embeddings
      - Milvus search
      - Rule-based conversion to JSON filters
    """
    user_query = state["user_query"]

    # 1) Vector search against schema collection
    schema_hits = search_schema_in_milvus(user_query, k=TOP_K_SCHEMA)

    # 2) Build structured query JSON (no LLM)
    structured = build_structured_query(user_query, schema_hits)

    return {
        "schema_hits": schema_hits,
        "structured_query": structured,
    }


# -------------------------
# BUILD GRAPH
# -------------------------
builder = StateGraph(RewriteState)

builder.add_node("rewrite_query", rewrite_query_node)
builder.add_edge(START, "rewrite_query")
builder.add_edge("rewrite_query", END)

rewrite_graph = builder.compile()


if __name__ == "__main__":
    # quick manual test
    test_state: RewriteState = {
        "user_query": "Get all PAN and GSTIN fields for billing address"
    }

    result = rewrite_graph.invoke(test_state)

    from pprint import pprint

    print("\n=== Schema hits ===")
    pprint(result["schema_hits"])

    print("\n=== Structured query ===")
    pprint(result["structured_query"])











# rewrite_query_ollama_bgem3.py
"""
LangGraph node for query rewrite using:
- Ollama bge-m3 embeddings  (http://localhost:11434/api/embeddings)
- Milvus for schema search
- Rule-based conversion to JSON filters (NO GPT / NO OpenAI)

Collection used: form_schema_index

Expected fields in `form_schema_index`:
  - Question_label (VARCHAR)
  - Prop           (VARCHAR)
  - Section        (VARCHAR)
  - Subsection     (VARCHAR)
  - dense          (FLOAT_VECTOR, 1024)   <-- from Ollama bge-m3
"""

from typing import List, Dict, Any
from typing_extensions import TypedDict
import os
import requests

from langgraph.graph import StateGraph, START, END
from pymilvus import MilvusClient


# -------------------------
# CONFIG
# -------------------------
MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
SCHEMA_COLLECTION = os.getenv("SCHEMA_COLLECTION", "form_schema_index")

# Ollama settings
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/embeddings")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "bge-m3")

DENSE_DIM = 1024
TOP_K_SCHEMA = 8          # how many schema fields to consider
SCORE_THRESHOLD = 0.0     # keep all; bump this up to drop weak matches


# -------------------------
# LANGGRAPH STATE
# -------------------------
class RewriteState(TypedDict):
    # Input
    user_query: str

    # Outputs (filled by node)
    schema_hits: List[Dict[str, Any]]
    structured_query: Dict[str, Any]


# -------------------------
# MILVUS CLIENT
# -------------------------
client = MilvusClient(uri=MILVUS_URI)


# -------------------------
# OLLAMA bge-m3 EMBEDDINGS
# -------------------------
def get_bgem3_embedding(text: str) -> List[float]:
    """
    Call Ollama /api/embeddings with bge-m3 and return a 1024-dim vector.

    This MUST match how you embedded when ingesting form_schema_index.
    """
    resp = requests.post(
        OLLAMA_URL,
        json={
            "model": OLLAMA_MODEL,
            "prompt": text,   # use "prompt" to be consistent with your ingest script
        },
        timeout=120,
    )
    resp.raise_for_status()
    data = resp.json()

    emb = data.get("embedding")
    if emb is None:
        raise RuntimeError(f"No 'embedding' field in Ollama response: {data}")

    if len(emb) != DENSE_DIM:
        print(f"âš ï¸ Warning: expected dim {DENSE_DIM}, got {len(emb)}")

    return emb


# -------------------------
# SCHEMA SEARCH
# -------------------------
def search_schema_in_milvus(query: str, k: int = TOP_K_SCHEMA) -> List[Dict[str, Any]]:
    """
    Vector search against form_schema_index using bge-m3 embedding from Ollama.
    Returns a list[dict] with label/prop/section/subsection + score.
    """
    q_vec = get_bgem3_embedding(query)

    res = client.search(
        collection_name=SCHEMA_COLLECTION,
        data=[q_vec],
        anns_field="dense",
        limit=k,
        output_fields=["Question_label", "Prop", "Section", "Subsection"],
        search_params={
            "metric_type": "COSINE",
            "params": {},
        },
    )

    hits: List[Dict[str, Any]] = []
    if not res:
        return hits

    # res is List[List[hit]]; one query vector => res[0]
    for hit in res[0]:
        entity = hit.get("entity", {})
        score = hit.get("distance")  # for COSINE this is similarity / distance depending on Milvus config
        hits.append(
            {
                "id": hit.get("id"),
                "score": score,
                "Question_label": entity.get("Question_label", ""),
                "Prop": entity.get("Prop", ""),
                "Section": entity.get("Section", ""),
                "Subsection": entity.get("Subsection", ""),
            }
        )

    return hits


# -------------------------
# SIMPLE RULE-BASED "REWRITE"
# -------------------------
def build_structured_query(
    user_query: str,
    schema_hits: List[Dict[str, Any]],
    score_threshold: float = SCORE_THRESHOLD,
) -> Dict[str, Any]:
    """
    Convert top schema hits into a simple structured JSON filter.

    No LLM here: we just pick the best matches from Milvus.
    """
    filters: List[Dict[str, str]] = []

    for h in schema_hits:
        score = h.get("score") or 0.0
        # If you want to drop weak matches, set SCORE_THRESHOLD > 0
        if score < score_threshold:
            continue

        filters.append(
            {
                "prop": h.get("Prop", ""),
                "section": h.get("Section", "") or "",
                "subsection": h.get("Subsection", "") or "",
            }
        )

    # Very basic "rewrite" string (just for logging/debug)
    if filters:
        prop_list = [f["prop"] for f in filters if f["prop"]]
        rewrite_str = (
            f"Use fields: {', '.join(prop_list)} for the query: {user_query}"
            if prop_list
            else user_query
        )
    else:
        rewrite_str = user_query

    return {
        "filters": filters,
        "original_query": user_query,
        "rewrite": rewrite_str,
    }


# -------------------------
# LANGGRAPH NODE
# -------------------------
def rewrite_query_node(state: RewriteState) -> Dict[str, Any]:
    """
    Node:
      input : state['user_query']
      output: state['schema_hits'], state['structured_query']

    Uses only:
      - Ollama bge-m3 embeddings
      - Milvus search
      - Rule-based conversion to JSON filters
    """
    user_query = state["user_query"]

    # 1) Vector search on schema collection
    schema_hits = search_schema_in_milvus(user_query, k=TOP_K_SCHEMA)

    # 2) Build structured query JSON
    structured = build_structured_query(user_query, schema_hits)

    return {
        "schema_hits": schema_hits,
        "structured_query": structured,
    }


# -------------------------
# BUILD GRAPH
# -------------------------
builder = StateGraph(RewriteState)

builder.add_node("rewrite_query", rewrite_query_node)
builder.add_edge(START, "rewrite_query")
builder.add_edge("rewrite_query", END)

rewrite_graph = builder.compile()


if __name__ == "__main__":
    # Quick manual test
    test_state: RewriteState = {
        "user_query": "Get all PAN and GSTIN fields for billing address"
    }

    result = rewrite_graph.invoke(test_state)

    from pprint import pprint

    print("\n=== Schema hits ===")
    pprint(result["schema_hits"])

    print("\n=== Structured query ===")
    pprint(result["structured_query"])













from pymilvus import MilvusClient

client = MilvusClient(uri="http://localhost:19530")

def get_results_in_prop_order(structured_query, limit=10):
    ordered_results = []

    for f in structured_query["filters"]:
        expr_parts = []

        if f.get("prop"):
            expr_parts.append(f"Prop == '{f['prop']}'")
        if f.get("section"):
            expr_parts.append(f"Section == '{f['section']}'")
        if f.get("subsection"):
            expr_parts.append(f"Subsection == '{f['subsection']}'")

        expr = " and ".join(expr_parts)
        # Query only this propâ€™s rows
        rows = client.query(
            collection_name="form_field_values",   # your main collection
            filter=expr,
            output_fields=[
                "Question_label",
                "Prop",
                "Section",
                "Subsection",
                "Value",
            ],
            limit=limit,  # per-prop cap
        )

        ordered_results.extend(rows)

        if len(ordered_results) >= limit:
            break

    return ordered_results[:limit]


results = get_results_in_prop_order(structured_query, limit=10)

for r in results:
    print(r["Prop"], "|", r["Question_label"], "|", r.get("Value"))










def build_expr(structured_query: dict) -> str:
    filters = structured_query.get("filters", [])
    if not filters:
        return ""

    groups = []

    for f in filters:
        conds = []

        prop = f.get("prop")
        section = f.get("section")
        subsection = f.get("subsection")

        if prop:
            conds.append(f"Prop == '{prop}'")
        if section:
            conds.append(f"Section == '{section}'")
        if subsection:
            conds.append(f"Subsection == '{subsection}'")

        if conds:
            # (Prop == '...' and Section == '...' and Subsection == '...')
            groups.append("(" + " and ".join(conds) + ")")

    # ( ... ) or ( ... ) or ( ... )
    expr = " or ".join(groups)
    return expr









total = 0
offset = 0
while True:
    batch = client.query(collection_name, filter=expr, output_fields=["id"], limit=1000, offset=offset)
    total += len(batch)
    if len(batch) < 1000:
        break
    offset += 1000
print(total)









from typing import Annotated, TypedDict
from langgraph.graph import add_messages

def limited_add_messages(prev, new):
    """
    Combine old + new messages and keep only the last 7.
    This replaces the default add_messages reducer.
    """
    combined = add_messages(prev, new)
    return combined[-7:]

class State(TypedDict):
    # This tells LangGraph:
    # - this channel is called "messages"
    # - when a node returns {"messages": [...]}, merge using limited_add_messages
    messages: Annotated[list, limited_add_messages]




from langgraph.graph import StateGraph, START, END

builder = StateGraph(State)

# youâ€™ll add nodes nextâ€¦


from langchain.schema import HumanMessage, AIMessage

def rewrite_node(state: State):
    messages = state["messages"]

    # last message is the current user query (assuming you always append user last)
    last_msg = messages[-1]
    user_query = last_msg["content"] if isinstance(last_msg, dict) else last_msg.content

    # do your rewrite logic here (LLM, rules, etc.)
    structured_query = {
        "original_query": user_query,
        # ... field_filters, record_filters, value_slots, etc.
    }

    # if you want to also store an assistant â€œrewrite doneâ€ message in history:
    assistant_msg = {
        "role": "assistant",
        "content": f"[rewrite] {structured_query}"
    }

    # IMPORTANT: return *only new messages*; reducer will merge and trim to 7
    return {
        "messages": [assistant_msg],
        # you can also put structured_query in some other channel if you want
    }



def rewrite_node(state: State):
    messages = state["messages"]
    last_msg = messages[-1]
    user_query = last_msg.content

    # ...
    assistant_msg = AIMessage(content="[rewrite done]")
    return {"messages": [assistant_msg]}




def retrieve_node(state: State):
    messages = state["messages"]
    last_msg = messages[-1]   # still the most recent (user or assistant)

    # you can also inspect previous messages:
    # messages[-2], messages[-3], etc. (up to 7 stored)

    # Do retrieval, then add an assistant message with answer
    answer = "..."  # from LLM or RAG

    assistant_msg = {
        "role": "assistant",
        "content": answer,
    }

    return {"messages": [assistant_msg]}



builder.add_node("rewrite", rewrite_node)
builder.add_node("retrieve", retrieve_node)

builder.add_edge(START, "rewrite")
builder.add_edge("rewrite", "retrieve")
builder.add_edge("retrieve", END)

graph = builder.compile(
    # you can still use SqliteSaver + store if you want persistence
    checkpointer=checkpointer,
    store=memory_store,
)



graph.invoke(
    {"messages": [{"role": "user", "content": "get addresses for client abc"}]},
    config={"configurable": {"thread_id": "t1"}},
)




----++----++++-----------------

from typing import Annotated, TypedDict
from langgraph.graph import add_messages

def limited_add_messages(prev, new):
    """Merge old + new messages, keep only last 7."""
    combined = add_messages(prev, new)
    return combined[-7:]

class State(TypedDict):
    # LangGraph will use limited_add_messages whenever a node
    # returns {"messages": [...]} to merge with existing messages.
    messages: Annotated[list, limited_add_messages]


from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage

llm = ChatOpenAI(model="gpt-4o-mini")  # or your LLM client

def rewrite_node(state: State) -> dict:
    messages = state["messages"]
    # last message is the latest user input
    last_user = messages[-1]

    # build a tiny text context from previous messages (last 5)
    history_text = ""
    for m in messages[-5:-1]:  # all but last
        role = m["role"]
        content = m["content"]
        history_text += f"{role}: {content}\n"

    user_query = last_user["content"]

    chat_messages = [
        SystemMessage(content=(
            "You rewrite user queries for a retrieval system.\n"
            "Use the conversation history to resolve references like "
            "'above', 'this client', 'same as before'.\n"
            "Return ONLY the cleaned query text, no explanation."
        )),
        HumanMessage(content=f"History:\n{history_text}\n\nCurrent query:\n{user_query}")
    ]

    resp = llm.invoke(chat_messages)
    cleaned_query = resp.content.strip()

    # Option 1: store the cleaned query as an assistant message
    rewrite_msg = {
        "role": "assistant",
        "content": f"[rewrite] {cleaned_query}",
    }

    # Option 2: also (or instead) store it in some other state key if you expand State later.

    return {"messages": [rewrite_msg]}


def answer_node(state: State) -> dict:
    messages = state["messages"]

    # pick the last rewrite (assistant) and last user query
    last_user = None
    last_rewrite = None
    for m in reversed(messages):
        if last_user is None and m["role"] == "user":
            last_user = m
        if last_rewrite is None and m["role"] == "assistant" and m["content"].startswith("[rewrite]"):
            last_rewrite = m
        if last_user and last_rewrite:
            break

    user_query = last_user["content"] if last_user else ""
    cleaned_query = last_rewrite["content"].replace("[rewrite]", "", 1).strip() if last_rewrite else user_query

    # now use cleaned_query + maybe history for final answer
    chat_messages = [
        SystemMessage(content="You are a helpful assistant answering user questions."),
        HumanMessage(content=f"Original question: {user_query}\n\nRewritten query: {cleaned_query}")
    ]

    resp = llm.invoke(chat_messages)
    answer = resp.content.strip()

    answer_msg = {
        "role": "assistant",
        "content": answer,
    }

    return {"messages": [answer_msg]}


# Build the graph
builder = StateGraph(State)
builder.add_node("rewrite", rewrite_node)
builder.add_node("answer", answer_node)

builder.add_edge(START, "rewrite")
builder.add_edge("rewrite", "answer")
builder.add_edge("answer", END)

graph = builder.compile()  # ðŸ‘ˆ no checkpointer, no store




# initial empty state
state: State = {"messages": []}

while True:
    user_input = input("You: ")
    if not user_input:
        break

    # 1) append user message to state
    state["messages"].append({"role": "user", "content": user_input})

    # 2) run the graph; it will:
    #    - call rewrite_node â†’ adds [rewrite] message
    #    - call answer_node  â†’ adds answer message
    state = graph.invoke(state)

    # 3) get the last assistant answer (the final message)
    last_msg = state["messages"][-1]
    print("Bot:", last_msg["content"])




history_text = ""
for m in messages[-5:-1]:
    role = m["role"]
    content = m["content"]
    history_text += f"{role}: {content}\n"









from langchain_community.vectorstores import Milvus as LC_Milvus

vectorstore = LC_Milvus(
    embedding_function=embeddings,        # your Ollama BGE-M3 embedder
    connection_args={"host": "localhost", "port": "19530"},
    collection_name="forms_hybrid",       # your existing collection name
)









You could try creating a temporary "thinking" message when the user sends a prompt, then update that same message with the actual response when it comes back. Something like this might work:
```javascript
const [messages, setMessages] = useState([]);

function handleSend(userInput) {
    const tempId = generateUniqueId();
    
    setMessages(old => [
        ...old,
        { id: tempId, role: "assistant", content: "Thinking...", isPending: true }
    ]);
    
    fetch("/api/...", {
        method: "POST",
        body: JSON.stringify({ input: userInput, tempId }),
        headers: { "Content-Type": "application/json" }
    })
    .then(response => response.json())
    .then(data => {
        setMessages(old => 
            old.map(msg => 
                msg.id === tempId 
                    ? { ...msg, content: data.content, isPending: false }
                    : msg
            )
        );
    });