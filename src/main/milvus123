#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25_crf_intpk_pandas_dir.py

- Reads ALL CSV files in a directory
- PK = INT64 → id = <CRF_INT><row_sequence>  (e.g. 10011, 10012)
- CRF integer detected per file
- Uses pandas + rank-bm25 + Ollama bge-m3
- Inserts one row at a time (safe for Milvus)
"""

import os
import json
import argparse
import requests
import pandas as pd
from typing import Dict, Any, List
from glob import glob

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi


# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

BGE_DIM = 1024
DEFAULT_STATUS = "active"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- HELPERS --------
def normalize_value(val: str) -> str:
    return (str(val) or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = str(val).strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: pd.Series) -> str:
    parts = []
    if pd.notna(row.get("question_label")):
        parts.append(f"label: {row['question_label']}")
    if pd.notna(row.get("Prop")):
        parts.append(f"property: {row['Prop']}")
    if pd.notna(row.get("Section")):
        parts.append(f"section: {row['Section']}")
    if pd.notna(row.get("Sub_section")):
        parts.append(f"subsection: {row['Sub_section']}")
    if pd.notna(row.get("Json_pointer")):
        parts.append(f"pointer: {row['Json_pointer']}")
    return " ".join(parts)


def find_crf_int(df: pd.DataFrame) -> int:
    mask = (
        (df["question_label"].astype(str).str.lower() == "crf")
        | (df["Prop"].astype(str).str.lower() == "crf")
    )
    subset = df.loc[mask]
    if not subset.empty:
        val = subset.iloc[0]["Value"]
        try:
            return int(val)
        except Exception:
            pass
    return 1


# -------- COLLECTION --------
def ensure_collection(client: MilvusClient):
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)
    schema.add_field("id", DataType.INT64, is_primary=True)
    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)
    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)

    idx = client.prepare_index_params()
    idx.add_index("dense", "dense_idx", "AUTOINDEX", "COSINE")
    idx.add_index(
        "sparse",
        "sparse_idx",
        "SPARSE_INVERTED_INDEX",
        "IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    idx.add_index("value_vec", "value_vec_idx", "AUTOINDEX", "COSINE")

    client.create_collection(COLLECTION, schema=schema, index_params=idx)
    client.load_collection(COLLECTION)


# -------- INGEST ONE FILE --------
def ingest_one_csv(client: MilvusClient, csv_path: str):
    df = pd.read_csv(csv_path, encoding="utf-8-sig").fillna("")
    print(f"\n[+] Reading {os.path.basename(csv_path)} ({len(df)} rows)")

    crf_int = find_crf_int(df)
    print(f"    CRF base: {crf_int}")

    schema_texts = df.apply(build_schema_text, axis=1).tolist()
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    count = 0
    for idx, (i, row) in enumerate(df.iterrows(), start=1):
        ql = row.get("question_label", "")
        prop = row.get("Prop", "")
        val = row.get("Value", "")
        ptr = row.get("Json_pointer", "")
        sec = row.get("Section", "")
        sub = row.get("Sub_section", "")
        val_norm = normalize_value(val)

        schema_text = schema_texts[idx - 1]
        doc_tokens = schema_text.lower().split()

        schema_dense = ollama_embed(schema_text)

        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc: Dict[str, int] = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        pk_int = int(f"{crf_int}{idx}")

        one_row = {
            "id": pk_int,
            "question_label": ql,
            "prop": prop,
            "value_text": val_norm,
            "json_pointer": ptr,
            "section": sec,
            "subsection": sub,
            "status": DEFAULT_STATUS,
            "dense": schema_dense,
            "sparse": sparse_vec,
            "value_vec": value_dense,
        }

        client.insert(COLLECTION, [one_row])
        count += 1

        if count % 100 == 0:
            client.flush(COLLECTION)
            print(f"    Inserted {count} rows...")

    client.flush(COLLECTION)
    print(f"[OK] {os.path.basename(csv_path)} → {count} rows ingested (CRF={crf_int})")

    return vocab, bm25


# -------- MAIN --------
def ingest_directory(client: MilvusClient, dir_path: str):
    ensure_collection(client)
    csv_files = sorted(glob(os.path.join(dir_path, "*.csv")))
    if not csv_files:
        print(f"[!] No CSV files found in {dir_path}")
        return

    print(f"[i] Found {len(csv_files)} CSV files in {dir_path}")

    last_vocab, last_bm25 = None, None
    for csv_file in csv_files:
        vocab, bm25 = ingest_one_csv(client, csv_file)
        last_vocab, last_bm25 = vocab, bm25

    # Save last BM25 model (can be shared for similar schema)
    if last_vocab and last_bm25:
        bm25_state = {
            "k1": last_bm25.k1,
            "b": last_bm25.b,
            "avgdl": last_bm25.avgdl,
            "idf": last_bm25.idf,
            "vocab": last_vocab,
        }
        with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
            json.dump(bm25_state, f, ensure_ascii=False, indent=2)
        print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dir", required=True, help="Directory containing CSV files")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_directory(client, args.dir)


if __name__ == "__main__":
    main()