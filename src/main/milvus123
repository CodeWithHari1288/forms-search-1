#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25_crf_intpk_pandas_dir.py

- Reads ALL CSV files in a directory
- PK = INT64 ‚Üí id = <CRF_INT><row_sequence>  (e.g. 10011, 10012)
- CRF integer detected per file
- Uses pandas + rank-bm25 + Ollama bge-m3
- Inserts one row at a time (safe for Milvus)
"""

import os
import json
import argparse
import requests
import pandas as pd
from typing import Dict, Any, List
from glob import glob

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi


# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

BGE_DIM = 1024
DEFAULT_STATUS = "active"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- HELPERS --------
def normalize_value(val: str) -> str:
    return (str(val) or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = str(val).strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: pd.Series) -> str:
    parts = []
    if pd.notna(row.get("question_label")):
        parts.append(f"label: {row['question_label']}")
    if pd.notna(row.get("Prop")):
        parts.append(f"property: {row['Prop']}")
    if pd.notna(row.get("Section")):
        parts.append(f"section: {row['Section']}")
    if pd.notna(row.get("Sub_section")):
        parts.append(f"subsection: {row['Sub_section']}")
    if pd.notna(row.get("Json_pointer")):
        parts.append(f"pointer: {row['Json_pointer']}")
    return " ".join(parts)


def find_crf_int(df: pd.DataFrame) -> int:
    mask = (
        (df["question_label"].astype(str).str.lower() == "crf")
        | (df["Prop"].astype(str).str.lower() == "crf")
    )
    subset = df.loc[mask]
    if not subset.empty:
        val = subset.iloc[0]["Value"]
        try:
            return int(val)
        except Exception:
            pass
    return 1


# -------- COLLECTION --------
def ensure_collection(client: MilvusClient):
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)
    schema.add_field("id", DataType.INT64, is_primary=True)
    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)
    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)

    idx = client.prepare_index_params()
    idx.add_index("dense", "dense_idx", "AUTOINDEX", "COSINE")
    idx.add_index(
        "sparse",
        "sparse_idx",
        "SPARSE_INVERTED_INDEX",
        "IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    idx.add_index("value_vec", "value_vec_idx", "AUTOINDEX", "COSINE")

    client.create_collection(COLLECTION, schema=schema, index_params=idx)
    client.load_collection(COLLECTION)


# -------- INGEST ONE FILE --------
def ingest_one_csv(client: MilvusClient, csv_path: str):
    df = pd.read_csv(csv_path, encoding="utf-8-sig").fillna("")
    print(f"\n[+] Reading {os.path.basename(csv_path)} ({len(df)} rows)")

    crf_int = find_crf_int(df)
    print(f"    CRF base: {crf_int}")

    schema_texts = df.apply(build_schema_text, axis=1).tolist()
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    count = 0
    for idx, (i, row) in enumerate(df.iterrows(), start=1):
        ql = row.get("question_label", "")
        prop = row.get("Prop", "")
        val = row.get("Value", "")
        ptr = row.get("Json_pointer", "")
        sec = row.get("Section", "")
        sub = row.get("Sub_section", "")
        val_norm = normalize_value(val)

        schema_text = schema_texts[idx - 1]
        doc_tokens = schema_text.lower().split()

        schema_dense = ollama_embed(schema_text)

        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc: Dict[str, int] = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        pk_int = int(f"{crf_int}{idx}")

        one_row = {
            "id": pk_int,
            "question_label": ql,
            "prop": prop,
            "value_text": val_norm,
            "json_pointer": ptr,
            "section": sec,
            "subsection": sub,
            "status": DEFAULT_STATUS,
            "dense": schema_dense,
            "sparse": sparse_vec,
            "value_vec": value_dense,
        }

        client.insert(COLLECTION, [one_row])
        count += 1

        if count % 100 == 0:
            client.flush(COLLECTION)
            print(f"    Inserted {count} rows...")

    client.flush(COLLECTION)
    print(f"[OK] {os.path.basename(csv_path)} ‚Üí {count} rows ingested (CRF={crf_int})")

    return vocab, bm25


# -------- MAIN --------
def ingest_directory(client: MilvusClient, dir_path: str):
    ensure_collection(client)
    csv_files = sorted(glob(os.path.join(dir_path, "*.csv")))
    if not csv_files:
        print(f"[!] No CSV files found in {dir_path}")
        return

    print(f"[i] Found {len(csv_files)} CSV files in {dir_path}")

    last_vocab, last_bm25 = None, None
    for csv_file in csv_files:
        vocab, bm25 = ingest_one_csv(client, csv_file)
        last_vocab, last_bm25 = vocab, bm25

    # Save last BM25 model (can be shared for similar schema)
    if last_vocab and last_bm25:
        bm25_state = {
            "k1": last_bm25.k1,
            "b": last_bm25.b,
            "avgdl": last_bm25.avgdl,
            "idf": last_bm25.idf,
            "vocab": last_vocab,
        }
        with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
            json.dump(bm25_state, f, ensure_ascii=False, indent=2)
        print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dir", required=True, help="Directory containing CSV files")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_directory(client, args.dir)


if __name__ == "__main__":
    main()












#!/usr/bin/env python3
"""
retrieve_milvus_v3_ollama_bm25_rrf.py

Hybrid retrieval (dense + BM25 sparse) from Milvus, matching:
- collection: forms_hybrid_v3
- schema: INT64 PK, question_label, prop, value_text, json_pointer, section, subsection, status,
          dense (1024), sparse, value_vec (1024)
- query encoder: Ollama bge-m3
- sparse leg: BM25, loaded from bm25_model.json
- ranker: RRF (as requested)

Usage:
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address city"
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address" --section "Address" --value "hyderabad"
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "gstin" --status "active" --topk 20
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address hyderabad" --value-fuzzy "hyderabad"

Notes:
- If bm25_model.json is missing, we fall back to dense-only.
- If you want fuzzy on value, pass --value-fuzzy; otherwise we just filter exact on value_text.
"""

import os
import json
import argparse
import requests
from typing import Dict, Any, List, Optional

from pymilvus import (
    MilvusClient,
    AnnSearchRequest,
    RRFRanker,
)

# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=20)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- BM25 LOAD / QUERY ‚Üí SPARSE VEC --------
def load_bm25_model(path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def bm25_query_to_sparse(query: str, bm25_state: Dict[str, Any]) -> Dict[int, float]:
    """
    Convert a text query into the same sparse format we used when ingesting.

    We approximate BM25 scoring for the query terms using the stored idf, k1, b, avgdl.
    Query is treated as a short doc.
    """
    tokens = query.lower().split()
    if not tokens:
        return {}

    k1 = bm25_state["k1"]
    b = bm25_state["b"]
    avgdl = bm25_state["avgdl"]
    idf_map = bm25_state["idf"]
    vocab = bm25_state["vocab"]

    # query treated as doc
    doc_len = len(tokens)
    tf_in_q = {}
    for t in tokens:
        tf_in_q[t] = tf_in_q.get(t, 0) + 1

    sparse: Dict[int, float] = {}
    for tok, tf in tf_in_q.items():
        if tok not in idf_map:
            continue
        idf = idf_map[tok]
        score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
        if tok in vocab:
            term_id = vocab[tok]
            sparse[term_id] = float(score)

    return sparse


# -------- FILTER BUILDER --------
def build_expr(section: str, subsection: str, status: str, value_text: str) -> str:
    clauses = []
    if section:
        clauses.append(f'section == "{section}"')
    if subsection:
        clauses.append(f'subsection == "{subsection}"')
    if status:
        clauses.append(f'status == "{status}"')
    if value_text:
        # we normalized to lowercase at ingest
        vt = value_text.strip().lower()
        clauses.append(f'value_text == "{vt}"')
    return " and ".join(clauses)


# -------- MAIN RETRIEVE --------
def retrieve(
    query: str,
    section: str = "",
    subsection: str = "",
    status: str = "",
    value_text: str = "",
    value_fuzzy: str = "",
    topk: int = 20,
):
    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)

    # embed query for dense leg
    q_dense = ollama_embed(query)

    # try to load BM25 model
    bm25_state = load_bm25_model(BM25_MODEL_FILE)
    q_sparse = None
    if bm25_state is not None:
        q_sparse = bm25_query_to_sparse(query, bm25_state)

    expr = build_expr(section, subsection, status, value_text)

    reqs: List[AnnSearchRequest] = []

    # 1) DENSE leg
    req_dense = AnnSearchRequest(
        data=[q_dense],
        anns_field="dense",
        limit=topk,
        param={"metric_type": "COSINE", "params": {}},
        expr=expr if expr else None,
    )
    reqs.append(req_dense)

    # 2) SPARSE (BM25) leg ‚Äî only if we have the model
    if q_sparse:
        req_sparse = AnnSearchRequest(
            data=[q_sparse],
            anns_field="sparse",
            limit=topk,
            param={
                "metric_type": "IP",
                "params": {
                    # this is optional; trims tiny weights
                    "drop_ratio_search": 0.2
                },
            },
            expr=expr if expr else None,
        )
        reqs.append(req_sparse)

    # 3) OPTIONAL: value fuzzy leg (search in value_vec)
    if value_fuzzy:
        q_val_dense = ollama_embed(value_fuzzy)
        req_val = AnnSearchRequest(
            data=[q_val_dense],
            anns_field="value_vec",
            limit=topk,
            param={"metric_type": "COSINE", "params": {}},
            # we probably still want section/status filter here:
            expr=build_expr(section, subsection, status, ""),  # no exact value filter
        )
        reqs.append(req_val)

    # RRF: robust across unequal legs
    ranker = RRFRanker()  # default k=60; can tune if needed

    res = client.hybrid_search(
        collection_name=COLLECTION,
        reqs=reqs,
        ranker=ranker,
        limit=topk,
        output_fields=[
            "question_label",
            "prop",
            "value_text",
            "json_pointer",
            "section",
            "subsection",
            "status",
        ],
    )

    # print results
    print(f"\n=== Results for query: {query!r} ===")
    for hits in res:  # we sent one query, so this is 1 list
        for h in hits:
            print("-" * 60)
            print(f"id:            {h.id}")
            print(f"distance:      {h.distance}")
            print(f"label:         {h.get('question_label')}")
            print(f"prop:          {h.get('prop')}")
            print(f"value_text:    {h.get('value_text')}")
            print(f"pointer:       {h.get('json_pointer')}")
            print(f"section:       {h.get('section')}  |  subsection: {h.get('subsection')}")
            print(f"status:        {h.get('status')}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--query", required=True, help="Natural language query, e.g. 'address city'")
    ap.add_argument("--section", default="", help="Filter by section")
    ap.add_argument("--subsection", default="", help="Filter by subsection")
    ap.add_argument("--status", default="", help="Filter by status")
    ap.add_argument(
        "--value",
        default="",
        help="Exact value_text filter (we stored values lowercased). e.g. --value hyderabad",
    )
    ap.add_argument(
        "--value-fuzzy",
        default="",
        help="If provided, we also search value_vec with this text (fuzzy match on values).",
    )
    ap.add_argument("--topk", type=int, default=20, help="How many results to return")
    args = ap.parse_args()

    retrieve(
        query=args.query,
        section=args.section,
        subsection=args.subsection,
        status=args.status,
        value_text=args.value,
        value_fuzzy=args.value_fuzzy,
        topk=args.topk,
    )


if __name__ == "__main__":
    main()




from langgraph.graph import StateGraph, MessagesState, END
from langchain.tools import tool

@tool("milvus_retrieve", return_direct=True)
def milvus_retrieve_tool(query: str) -> str:
    """Retrieve relevant form fields from Milvus hybrid index"""
    return retrieve_from_milvus(query)




def retrieve_from_milvus(query: str) -> str:
    res = milvus_client.hybrid_search(...query...)
    top_texts = [
        f"[{hit.id}] {hit.get('label')} ‚Üí {hit.get('value_text')}"
        for hit in res[0]
    ]
    return "\n".join(top_texts)








def format_hits_for_llm(hits, limit=5):
    blocks = []
    for i, h in enumerate(hits[:limit], start=1):
        blocks.append(
            "\n".join([
                f"[{i}]",
                f"question_label: {h.get('question_label')}",
                f"prop: {h.get('prop')}",
                f"value_text: {h.get('value_text')}",
                f"section: {h.get('section')}",
                f"subsection: {h.get('subsection')}",
                f"json_pointer: {h.get('json_pointer')}",
            ])
        )
    return "CONTEXT:\n" + "\n\n".join(blocks)























import json
import requests

OLLAMA_URL = "http://localhost:11434"   # change if different

def ollama_chat(model: str, content: str) -> str:
    r = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json={
            "model": model,
            "messages": [{"role": "user", "content": content}],
        },
        timeout=60,
    )
    r.raise_for_status()
    return r.json()["message"]["content"]


def rewrite_node(state):
    """
    LangGraph node:
    - input: state["messages"] (last is user)
    - output: same state + state["search_spec"]
    """
    user_q = state["messages"][-1]["content"]

    prompt = f"""
You convert a natural language query about FORM FIELDS into JSON for search.

Each row in the DB has these columns:
- question_label
- prop
- value_text
- section
- subsection
- json_pointer
- form_id
- status

Return JSON ONLY in this shape:

{{
  "semantic_query": "text to embed for semantic search",
  "section": null or "Address" or "KYC" or "Contact",
  "subsection": null or "Current" or "Billing" or "Mailing",
  "form_id": null or "Form A" or "A" or "B",
  "value_eq": null or "hyderabad" or "IN",
  "limit": 10
}}

Rules:
- If the user mentions a form, set form_id.
- If the user mentions a section, set section.
- If the user mentions an exact value (like a city), set value_eq (lowercase).
- If unsure, leave fields null.
- Always set limit (default 10).

User query: {user_q}
""".strip()

    raw = ollama_chat("llama3.2", prompt)

    # be defensive, the model may add text
    try:
        spec = json.loads(raw)
    except json.JSONDecodeError:
        spec = {
            "semantic_query": user_q,
            "section": None,
            "subsection": None,
            "form_id": None,
            "value_eq": None,
            "limit": 10,
        }

    # normalize value_eq to lowercase, if present
    if spec.get("value_eq"):
        spec["value_eq"] = str(spec["value_eq"]).lower().strip()

    # return updated state
    return {
        **state,
        "search_spec": spec,
    }


















# langgraph_milvus_rag.py

import json
import requests
from typing import Any, Dict, List, Optional

from pymilvus import MilvusClient, AnnSearchRequest, RRFRanker
from langgraph.graph import StateGraph, MessagesState

# -------------------------------------------------
# CONFIG
# -------------------------------------------------
OLLAMA_URL = "http://localhost:11434"
MILVUS_URI = "http://localhost:19530"
MILVUS_TOKEN = "root:Milvus"
COLLECTION = "forms_hybrid_v3"   # <- use your actual collection name


# -------------------------------------------------
# OLLAMA HELPERS
# -------------------------------------------------
def ollama_chat(model: str, content: str) -> str:
    """Call Ollama chat and return assistant text."""
    r = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json={"model": model, "messages": [{"role": "user", "content": content}]},
        timeout=60,
    )
    r.raise_for_status()
    data = r.json()
    return data["message"]["content"]


def ollama_embed_bgem3(text: str) -> List[float]:
    """Get dense embedding from Ollama bge-m3."""
    r = requests.post(
        f"{OLLAMA_URL}/api/embeddings",
        json={"model": "bge-m3", "prompt": text},
        timeout=60,
    )
    r.raise_for_status()
    data = r.json()
    return data["embedding"]  # list[float]


# -------------------------------------------------
# MILVUS CLIENT
# -------------------------------------------------
milvus = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)


# -------------------------------------------------
# 1) NODE: REWRITE (NL ‚Üí search spec)
# -------------------------------------------------
def rewrite_node(state: MessagesState) -> Dict[str, Any]:
    user_q = state["messages"][-1]["content"]

    prompt = f"""
You convert a natural language request about FORM FIELDS into a JSON search spec.

Each row in the DB has these columns:
- question_label
- prop
- value_text
- section
- subsection
- json_pointer
- form_id
- status

Return JSON ONLY, like:
{{
  "semantic_query": "...",   // text to embed (usually from the user's intent)
  "section": null or "Address",
  "subsection": null or "Current",
  "form_id": null or "Form A",
  "value_eq": null or "hyderabad",  // for exact value_text filter
  "limit": 10
}}

User query: {user_q}
""".strip()

    raw = ollama_chat("llama3.2", prompt)
    try:
        spec = json.loads(raw)
    except json.JSONDecodeError:
        # fallback if model gave extra text
        spec = {
            "semantic_query": user_q,
            "section": None,
            "subsection": None,
            "form_id": None,
            "value_eq": None,
            "limit": 10,
        }

    # store into state so next node can use
    return {
        **state,
        "search_spec": spec,
    }


# -------------------------------------------------
# (optional) sparse: for now we send empty dict
# later you plug your bm25 sparse builder here
# -------------------------------------------------
def build_sparse_from_bm25(query: str) -> Dict[int, float]:
    # placeholder: return {} ‚Üí Milvus will just use dense leg
    return {}


# -------------------------------------------------
# helper: format hits for LLM
# -------------------------------------------------
def format_hits_for_llm(res, limit: int = 6) -> str:
    # res is SearchResults -> take first hits (we queried 1 vector)
    hits = res[0]
    blocks = []
    for i, h in enumerate(hits[:limit], start=1):
        blocks.append(
            "\n".join(
                [
                    f"[{i}]",
                    f"question_label: {h.get('question_label')}",
                    f"prop: {h.get('prop')}",
                    f"value_text: {h.get('value_text')}",
                    f"section: {h.get('section')}",
                    f"subsection: {h.get('subsection')}",
                    f"json_pointer: {h.get('json_pointer')}",
                    f"form_id: {h.get('form_id')}",
                ]
            )
        )
    return "CONTEXT (retrieved rows):\n" + "\n\n".join(blocks)


# -------------------------------------------------
# 2) NODE: RETRIEVE (Milvus hybrid)
# -------------------------------------------------
def retrieve_node(state: MessagesState) -> Dict[str, Any]:
    spec = state.get("search_spec") or {}
    sem_q = spec.get("semantic_query") or state["messages"][-1]["content"]

    q_dense = ollama_embed_bgem3(sem_q)
    q_sparse = build_sparse_from_bm25(sem_q)  # can be {} for now

    # build filter expr
    clauses = ['status == "active"']  # you added this in ingest
    if spec.get("form_id"):
        clauses.append(f'form_id == "{spec["form_id"]}"')
    if spec.get("section"):
        clauses.append(f'section == "{spec["section"]}"')
    if spec.get("subsection"):
        clauses.append(f'subsection == "{spec["subsection"]}"')
    if spec.get("value_eq"):
        val = spec["value_eq"].lower().strip()
        clauses.append(f'value_text == "{val}"')

    expr = " and ".join(clauses)
    topk = int(spec.get("limit", 10))

    reqs = []

    # dense leg
    req_dense = AnnSearchRequest(
        data=[q_dense],
        anns_field="dense",
        param={"metric_type": "COSINE", "params": {}},
        limit=topk,
        expr=expr,
    )
    reqs.append(req_dense)

    # sparse leg (add only if not empty)
    if q_sparse:
        req_sparse = AnnSearchRequest(
            data=[q_sparse],
            anns_field="sparse",
            param={"metric_type": "IP", "params": {"drop_ratio_search": 0.2}},
            limit=topk,
            expr=expr,
        )
        reqs.append(req_sparse)

    # use RRF because you asked earlier
    ranker = RRFRanker()

    res = milvus.hybrid_search(
        collection_name=COLLECTION,
        reqs=reqs,
        ranker=ranker,
        limit=topk,
        output_fields=[
            "question_label",
            "prop",
            "value_text",
            "section",
            "subsection",
            "json_pointer",
            "form_id",
            "status",
        ],
    )

    ctx_text = format_hits_for_llm(res, limit=6)

    # üëá append as system message so next LLM sees it
    new_messages = list(state["messages"]) + [
        {
            "role": "system",
            "content": ctx_text,
        }
    ]

    return {
        **state,
        "messages": new_messages,
        "retrieval_raw": ctx_text,  # optional: keep raw
    }


# -------------------------------------------------
# 3) NODE: ANSWER (LLM over context)
# -------------------------------------------------
def answer_node(state: MessagesState) -> Dict[str, Any]:
    user_q = state["messages"][-1]["content"]

    # find latest system CONTEXT
    ctx_text = "CONTEXT: (empty)"
    for m in reversed(state["messages"]):
        if m["role"] == "system" and m["content"].startswith("CONTEXT"):
            ctx_text = m["content"]
            break

    prompt = f"""
You are a forms RAG assistant.

You are given some retrieved rows from the database.
Each row represents one field (question_label, prop, value_text, section, subsection, json_pointer).

You MUST answer ONLY from these rows.
If the user asks for "all related" information, return all rows that look related.
If the answer is not in the rows, say exactly: "Not found in retrieved rows."

{ctx_text}

USER QUESTION: {user_q}

Answer:
""".strip()

    answer = ollama_chat("llama3.2", prompt)

    new_messages = list(state["messages"]) + [
        {"role": "assistant", "content": answer}
    ]

    return {
        **state,
        "messages": new_messages,
    }


# -------------------------------------------------
# BUILD THE GRAPH
# -------------------------------------------------
def build_graph():
    g = StateGraph(MessagesState)
    g.add_node("rewrite", rewrite_node)
    g.add_node("retrieve", retrieve_node)
    g.add_node("answer", answer_node)

    # flow: user -> rewrite -> retrieve -> answer
    g.set_entry_point("rewrite")
    g.add_edge("rewrite", "retrieve")
    g.add_edge("retrieve", "answer")
    g.set_finish_point("answer")

    return g.compile()


if __name__ == "__main__":
    app = build_graph()
    # run once
    result = app.invoke(
        {
            "messages": [
                {"role": "user", "content": "Get me address related information from Form A"}
            ]
        }
    )
    # final assistant message
    for m in result["messages"]:
        if m["role"] == "assistant":
            print("\nASSISTANT:\n", m["content"])









#!/usr/bin/env python3
"""
Ingest script for Milvus standalone with dual collections:
1. form_schema_index  -> schema-level metadata (Question_label, Prop, Section, Subsection)
2. form_field_values  -> field-level hybrid (includes Value, Context, Pointer)

Uses BGE-M3 for dense + sparse embeddings (via FlagEmbedding).
"""

import os, glob, pandas as pd
from pymilvus import MilvusClient, DataType, FieldSchema, CollectionSchema, Collection
from FlagEmbedding import BGEM3FlagModel

# ----------------------------
# CONFIG
# ----------------------------
LITE_DB_FILE = "./forms.db"
CSV_DIR = "./csvs"
SCHEMA_COLLECTION = "form_schema_index"
FIELD_COLLECTION = "form_field_values"
DENSE_DIM = 1024
BATCH_SIZE = 100

REQUIRED_COLS = [
    "Question_label", "Prop", "Value",
    "Json_pointer", "Question_context",
    "Section", "Subsection"
]

# ----------------------------
# MODEL
# ----------------------------
print("Loading BGE-M3 model...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# ----------------------------
# CONNECT TO MILVUS
# ----------------------------
client = MilvusClient(uri=LITE_DB_FILE)

# ----------------------------
# COLLECTION DEFINITIONS
# ----------------------------
def ensure_collection(name: str, mode: str):
    """
    Create collection if not exists.
    mode = 'schema' | 'field'
    """
    if client.has_collection(name):
        print(f"‚úÖ Collection '{name}' already exists.")
        return

    base_fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="Question_label", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="Prop", dtype=DataType.VARCHAR, max_length=256),
        FieldSchema(name="Section", dtype=DataType.VARCHAR, max_length=256),
        FieldSchema(name="Subsection", dtype=DataType.VARCHAR, max_length=256),
        FieldSchema(name="dense", dtype=DataType.FLOAT_VECTOR, dim=DENSE_DIM),
        FieldSchema(name="sparse", dtype=DataType.SPARSE_FLOAT_VECTOR)
    ]

    if mode == "field":
        base_fields += [
            FieldSchema(name="Value", dtype=DataType.VARCHAR, max_length=2048),
            FieldSchema(name="Json_pointer", dtype=DataType.VARCHAR, max_length=512),
            FieldSchema(name="Question_context", dtype=DataType.VARCHAR, max_length=2048)
        ]

    schema = CollectionSchema(fields=base_fields, description=f"{mode} collection")
    client.create_collection(name=name, schema=schema)
    print(f"üÜï Created collection '{name}' with mode '{mode}'.")

# ----------------------------
# EMBEDDING HELPERS
# ----------------------------
def embed_row(label, prop, value=None):
    """Create dense+sparse embeddings for given text."""
    if value:
        text = f"{label} {prop} {value}"
    else:
        text = f"{label} {prop}"
    emb = model.encode(text, return_dense=True, return_sparse=True)
    return emb["dense_vecs"][0], emb["lexical_weights"][0]

# ----------------------------
# INGESTION
# ----------------------------
def ingest_csv_folder(folder):
    csv_files = glob.glob(os.path.join(folder, "*.csv"))
    if not csv_files:
        print("‚ùå No CSV files found.")
        return

    ensure_collection(SCHEMA_COLLECTION, "schema")
    ensure_collection(FIELD_COLLECTION, "field")

    for path in csv_files:
        print(f"üìÑ Reading {os.path.basename(path)} ...")
        df = pd.read_csv(path)
        missing = [c for c in REQUIRED_COLS if c not in df.columns]
        if missing:
            print(f"‚ö†Ô∏è Missing columns {missing} in {path}, skipping.")
            continue

        ingest_df(df)

def ingest_df(df: pd.DataFrame):
    schema_rows = []
    field_rows = []

    for _, row in df.iterrows():
        dense1, sparse1 = embed_row(row["Question_label"], row["Prop"])
        schema_rows.append({
            "Question_label": row["Question_label"],
            "Prop": row["Prop"],
            "Section": row["Section"],
            "Subsection": row["Subsection"],
            "dense": dense1,
            "sparse": sparse1
        })

        dense2, sparse2 = embed_row(row["Question_label"], row["Prop"], row["Value"])
        field_rows.append({
            "Question_label": row["Question_label"],
            "Prop": row["Prop"],
            "Value": row["Value"],
            "Json_pointer": row["Json_pointer"],
            "Question_context": row["Question_context"],
            "Section": row["Section"],
            "Subsection": row["Subsection"],
            "dense": dense2,
            "sparse": sparse2
        })

    print(f"‚¨ÜÔ∏è Inserting {len(schema_rows)} schema rows...")
    client.insert(collection_name=SCHEMA_COLLECTION, data=schema_rows)

    print(f"‚¨ÜÔ∏è Inserting {len(field_rows)} field rows...")
    client.insert(collection_name=FIELD_COLLECTION, data=field_rows)

    print("‚úÖ Done inserting batch.\n")

# ----------------------------
# MAIN
# ----------------------------
if __name__ == "__main__":
    ingest_csv_folder(CSV_DIR)
    print("‚úÖ All CSVs ingested successfully.")


