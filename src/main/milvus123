#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25_crf_intpk_pandas.py

- Reads CSV using pandas
- PK = INT64  (id = <CRF_INTEGER><sequence>)
  e.g. CRF=1001 → 10011, 10012, 10013 ...
- CRF value is detected automatically (from 'question_label' or 'Prop' == 'crf')
- Dense vectors from Ollama bge-m3
- Sparse vectors from rank-bm25
- Value vectors (short/non-JSON values) from Ollama
- Inserts rows one by one (safe for Milvus shape issues)
"""

import os
import json
import argparse
import requests
import pandas as pd
from typing import Dict, Any, List

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi

# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

BGE_DIM = 1024
DEFAULT_STATUS = "active"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- HELPERS --------
def normalize_value(val: str) -> str:
    return (str(val) or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = str(val).strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: pd.Series) -> str:
    parts = []
    if pd.notna(row.get("question_label")):
        parts.append(f"label: {row['question_label']}")
    if pd.notna(row.get("Prop")):
        parts.append(f"property: {row['Prop']}")
    if pd.notna(row.get("Section")):
        parts.append(f"section: {row['Section']}")
    if pd.notna(row.get("Sub_section")):
        parts.append(f"subsection: {row['Sub_section']}")
    if pd.notna(row.get("Json_pointer")):
        parts.append(f"pointer: {row['Json_pointer']}")
    return " ".join(parts)


def find_crf_int(df: pd.DataFrame) -> int:
    mask = (
        (df["question_label"].astype(str).str.lower() == "crf")
        | (df["Prop"].astype(str).str.lower() == "crf")
    )
    subset = df.loc[mask]
    if not subset.empty:
        val = subset.iloc[0]["Value"]
        try:
            return int(val)
        except Exception:
            pass
    return 1


# -------- COLLECTION --------
def ensure_collection(client: MilvusClient):
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)

    # PK is INT64 ✅
    schema.add_field("id", DataType.INT64, is_primary=True)

    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)

    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)

    idx = client.prepare_index_params()
    idx.add_index("dense", "dense_idx", "AUTOINDEX", "COSINE")
    idx.add_index(
        "sparse",
        "sparse_idx",
        "SPARSE_INVERTED_INDEX",
        "IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    idx.add_index("value_vec", "value_vec_idx", "AUTOINDEX", "COSINE")

    client.create_collection(COLLECTION, schema=schema, index_params=idx)
    client.load_collection(COLLECTION)


# -------- INGEST --------
def ingest_csv(client: MilvusClient, csv_path: str):
    ensure_collection(client)

    # 1) Read CSV via pandas
    df = pd.read_csv(csv_path, encoding="utf-8-sig").fillna("")
    print(f"[i] Loaded {len(df)} rows from {csv_path}")

    # 2) Detect CRF integer
    crf_int = find_crf_int(df)
    print(f"[i] Using CRF base integer: {crf_int}")

    # 3) Build schema texts
    schema_texts = df.apply(build_schema_text, axis=1).tolist()

    # 4) Build BM25 model
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    # 5) Iterate over DataFrame rows
    count = 0
    for idx, (i, row) in enumerate(df.iterrows(), start=1):
        ql = row.get("question_label", "")
        prop = row.get("Prop", "")
        val = row.get("Value", "")
        ptr = row.get("Json_pointer", "")
        sec = row.get("Section", "")
        sub = row.get("Sub_section", "")
        val_norm = normalize_value(val)

        schema_text = schema_texts[idx - 1]
        doc_tokens = schema_text.lower().split()

        # Dense vector for schema
        schema_dense = ollama_embed(schema_text)

        # Sparse BM25 vector
        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc: Dict[str, int] = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        # Dense vector for value
        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        # INT64 ID = CRF + sequence (no underscore)
        pk_int = int(f"{crf_int}{idx}")

        one_row = {
            "id": pk_int,
            "question_label": ql,
            "prop": prop,
            "value_text": val_norm,
            "json_pointer": ptr,
            "section": sec,
            "subsection": sub,
            "status": DEFAULT_STATUS,
            "dense": schema_dense,
            "sparse": sparse_vec,
            "value_vec": value_dense,
        }

        client.insert(COLLECTION, [one_row])
        count += 1
        if count % 100 == 0:
            client.flush(COLLECTION)
            print(f"[i] Inserted {count} rows...")

    client.flush(COLLECTION)
    print(f"[OK] Inserted {count} rows into {COLLECTION}")

    # 6) Save BM25 model
    bm25_state = {
        "k1": bm25.k1,
        "b": bm25.b,
        "avgdl": bm25.avgdl,
        "idf": bm25.idf,
        "vocab": vocab,
    }
    with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
        json.dump(bm25_state, f, ensure_ascii=False, indent=2)
    print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="CSV file to ingest")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_csv(client, args.csv)


if __name__ == "__main__":
    main()