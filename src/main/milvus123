#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25.py

Ingest CSV into Milvus with:
- dense: from Ollama bge-m3
- sparse: from rank-bm25 (real lexical weights)
- value_vec: from Ollama bge-m3, only for short/non-JSON values

Also writes a BM25 model file (JSON) so retrieval can reuse vocab & IDF.

Usage:
    python ingest_milvus_v3_ollama_bm25.py --csv ./forms.csv

Requires:
    pip install pymilvus rank-bm25 requests
"""

import os
import csv
import json
import argparse
import requests
from typing import Dict, Any, List

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi

# ---------------- CONFIG ----------------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")
BGE_DIM = 1024
DEFAULT_STATUS = "active"
OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")
BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# ---------------- OLLAMA EMBEDDING ----------------
def ollama_embed(text: str) -> List[float]:
    payload = {
        "model": OLLAMA_MODEL,
        "prompt": text,
    }
    resp = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    resp.raise_for_status()
    return resp.json()["embedding"]


# ---------------- HELPERS ----------------
def normalize_value(val: str) -> str:
    return (val or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = val.strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: Dict[str, str]) -> str:
    ql = row.get("question_label") or row.get("Question_label") or ""
    prop = row.get("Prop") or ""
    sec = row.get("Section") or ""
    sub = row.get("Sub_section") or row.get("Subsection") or ""
    ptr = row.get("Json_pointer") or row.get("Json_Pointer") or ""

    parts = []
    if ql:
        parts.append(f"label: {ql}")
    if prop:
        parts.append(f"property: {prop}")
    if sec:
        parts.append(f"section: {sec}")
    if sub:
        parts.append(f"subsection: {sub}")
    if ptr:
        parts.append(f"pointer: {ptr}")
    return " ".join(parts)


# ---------------- COLLECTION ----------------
def ensure_collection(client: MilvusClient):
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)

    schema.add_field("id", DataType.INT64, is_primary=True)
    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)

    # 3 vector legs
    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)      # schema dense
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)           # BM25 weights
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)  # value dense

    index_params = client.prepare_index_params()
    # dense
    index_params.add_index(
        field_name="dense",
        index_name="dense_idx",
        index_type="AUTOINDEX",
        metric_type="COSINE",
    )
    # sparse
    index_params.add_index(
        field_name="sparse",
        index_name="sparse_idx",
        index_type="SPARSE_INVERTED_INDEX",
        metric_type="IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    # value dense
    index_params.add_index(
        field_name="value_vec",
        index_name="value_vec_idx",
        index_type="AUTOINDEX",
        metric_type="COSINE",
    )

    client.create_collection(COLLECTION, schema=schema, index_params=index_params)
    client.load_collection(COLLECTION)


# ---------------- INGEST ----------------
def ingest_csv(client: MilvusClient, csv_path: str):
    ensure_collection(client)

    # 1) read all rows first (we need corpus for BM25)
    rows = []
    schema_texts = []
    with open(csv_path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
            schema_texts.append(build_schema_text(row))

    # 2) build BM25 over schema texts
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    # build vocab: token -> int
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    # 3) prepare insert buffer
    buf = {
        "id": [],
        "question_label": [],
        "prop": [],
        "value_text": [],
        "json_pointer": [],
        "section": [],
        "subsection": [],
        "status": [],
        "dense": [],
        "sparse": [],
        "value_vec": [],
    }

    # 4) for each row, build milvus-friendly sparse dict using BM25 formula
    pk = 1
    for row, doc_tokens in zip(rows, tokenized_docs):
        ql = row.get("question_label") or row.get("Question_label") or ""
        prop = row.get("Prop") or ""
        val = row.get("Value") or ""
        ptr = row.get("Json_pointer") or row.get("Json_Pointer") or ""
        sec = row.get("Section") or ""
        sub = row.get("Sub_section") or row.get("Subsection") or ""

        val_norm = normalize_value(val)

        # --- dense for schema (Ollama) ---
        schema_text = schema_texts[pk - 1]
        schema_dense = ollama_embed(schema_text)

        # --- sparse from BM25 ---
        # we re-compute BM25 *per term in this doc*
        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            # standard BM25 scoring per term
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        # --- value embedding (optional) ---
        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        buf["id"].append(pk)
        buf["question_label"].append(ql)
        buf["prop"].append(prop)
        buf["value_text"].append(val_norm)
        buf["json_pointer"].append(ptr)
        buf["section"].append(sec)
        buf["subsection"].append(sub)
        buf["status"].append(DEFAULT_STATUS)
        buf["dense"].append(schema_dense)
        buf["sparse"].append(sparse_vec)
        buf["value_vec"].append(value_dense)

        pk += 1

    client.insert(COLLECTION, buf)
    client.flush(COLLECTION)
    print(f"[OK] Inserted {len(buf['id'])} rows into {COLLECTION}")

    # 5) save BM25 model to JSON so retriever can reuse
    bm25_state = {
        "k1": bm25.k1,
        "b": bm25.b,
        "avgdl": bm25.avgdl,
        "idf": bm25.idf,      # dict: token -> idf
        "vocab": vocab,       # dict: token -> term_id
    }
    with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
        json.dump(bm25_state, f, ensure_ascii=False, indent=2)
    print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="CSV file to ingest")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_csv(client, args.csv)


if __name__ == "__main__":
    main()