#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25_crfpk_single.py

Same as before:
- PK = "<CRF_VALUE>_<row_number>"
- dense from Ollama (bge-m3)
- sparse from rank-bm25
- value_vec from Ollama for short values
BUT:
- we INSERT **one row at a time** to avoid
  "id should be varchar but got list" shape issues.

Run:
    python ingest_milvus_v3_ollama_bm25_crfpk_single.py --csv ./forms.csv
"""

import os
import csv
import json
import argparse
import requests
from typing import Dict, Any, List

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi

# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

BGE_DIM = 1024
DEFAULT_STATUS = "active"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- HELPERS --------
def normalize_value(val: str) -> str:
    return (val or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = val.strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: Dict[str, str]) -> str:
    ql = row.get("question_label") or row.get("Question_label") or ""
    prop = row.get("Prop") or ""
    sec = row.get("Section") or ""
    sub = row.get("Sub_section") or row.get("Subsection") or ""
    ptr = row.get("Json_pointer") or row.get("Json_Pointer") or ""

    parts = []
    if ql:
        parts.append(f"label: {ql}")
    if prop:
        parts.append(f"property: {prop}")
    if sec:
        parts.append(f"section: {sec}")
    if sub:
        parts.append(f"subsection: {sub}")
    if ptr:
        parts.append(f"pointer: {ptr}")
    return " ".join(parts)


def find_crf_value(rows: List[Dict[str, str]]) -> str:
    for r in rows:
        ql = (r.get("question_label") or r.get("Question_label") or "").strip().lower()
        prop = (r.get("Prop") or "").strip().lower()
        if ql == "crf" or prop == "crf":
            val = (r.get("Value") or "").strip()
            if val:
                return val
    return "CRF"


# -------- COLLECTION --------
def ensure_collection(client: MilvusClient):
    # if collection already exists with correct schema, we skip
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)

    # PK is VARCHAR so we can do CRF_1, CRF_2, ...
    schema.add_field("id", DataType.VARCHAR, is_primary=True, max_length=128)

    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)

    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)

    idx = client.prepare_index_params()
    idx.add_index("dense", "dense_idx", "AUTOINDEX", "COSINE")
    idx.add_index(
        "sparse",
        "sparse_idx",
        "SPARSE_INVERTED_INDEX",
        "IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    idx.add_index("value_vec", "value_vec_idx", "AUTOINDEX", "COSINE")

    client.create_collection(COLLECTION, schema=schema, index_params=idx)
    client.load_collection(COLLECTION)


# -------- INGEST --------
def ingest_csv(client: MilvusClient, csv_path: str):
    ensure_collection(client)

    # 1) read all rows first (we need whole corpus for BM25)
    rows: List[Dict[str, str]] = []
    schema_texts: List[str] = []
    with open(csv_path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for r in reader:
            rows.append(r)
            schema_texts.append(build_schema_text(r))

    # 2) find CRF value for this file
    crf_val = find_crf_value(rows).strip()
    if not crf_val:
        crf_val = "CRF"
    crf_val = crf_val.replace(" ", "_")  # sanitize

    # 3) build BM25 over schema texts
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    # 4) now insert ONE BY ONE
    count = 0
    for idx, (row, doc_tokens, schema_text) in enumerate(zip(rows, tokenized_docs, schema_texts), start=1):
        ql = row.get("question_label") or row.get("Question_label") or ""
        prop = row.get("Prop") or ""
        val = row.get("Value") or ""
        ptr = row.get("Json_pointer") or row.get("Json_Pointer") or ""
        sec = row.get("Section") or ""
        sub = row.get("Sub_section") or row.get("Subsection") or ""

        val_norm = normalize_value(val)

        # dense for schema
        schema_dense = ollama_embed(schema_text)

        # BM25 sparse for this doc
        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc: Dict[str, int] = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        # value_vec
        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        # PK for this row
        pk_str = f"{crf_val}_{idx}"

        one_row = {
            "id": pk_str,
            "question_label": ql,
            "prop": prop,
            "value_text": val_norm,
            "json_pointer": ptr,
            "section": sec,
            "subsection": sub,
            "status": DEFAULT_STATUS,
            "dense": schema_dense,
            "sparse": sparse_vec,
            "value_vec": value_dense,
        }

        # ðŸ‘‰ insert single row (as list-of-dicts)
        client.insert(COLLECTION, [one_row])
        count += 1

        # flush every 100 to keep things clean
        if count % 100 == 0:
            client.flush(COLLECTION)
            print(f"[i] Inserted {count} rows so far...")

    # final flush
    client.flush(COLLECTION)
    print(f"[OK] Inserted {count} rows from {csv_path} (CRF={crf_val})")

    # 5) save BM25 model (so retrieval can reuse vocab)
    bm25_state = {
        "k1": bm25.k1,
        "b": bm25.b,
        "avgdl": bm25.avgdl,
        "idf": bm25.idf,
        "vocab": vocab,
    }
    with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
        json.dump(bm25_state, f, ensure_ascii=False, indent=2)
    print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="CSV file to ingest")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_csv(client, args.csv)


if __name__ == "__main__":
    main()