#!/usr/bin/env python3
"""
ingest_milvus_v3_ollama_bm25_crf_intpk_pandas_dir.py

- Reads ALL CSV files in a directory
- PK = INT64 → id = <CRF_INT><row_sequence>  (e.g. 10011, 10012)
- CRF integer detected per file
- Uses pandas + rank-bm25 + Ollama bge-m3
- Inserts one row at a time (safe for Milvus)
"""

import os
import json
import argparse
import requests
import pandas as pd
from typing import Dict, Any, List
from glob import glob

from pymilvus import MilvusClient, DataType
from rank_bm25 import BM25Okapi


# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

BGE_DIM = 1024
DEFAULT_STATUS = "active"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=25)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- HELPERS --------
def normalize_value(val: str) -> str:
    return (str(val) or "").strip().lower()


def is_json_like(val: str) -> bool:
    if not val:
        return False
    v = str(val).strip()
    return v.startswith("{") or v.startswith("[")


def build_schema_text(row: pd.Series) -> str:
    parts = []
    if pd.notna(row.get("question_label")):
        parts.append(f"label: {row['question_label']}")
    if pd.notna(row.get("Prop")):
        parts.append(f"property: {row['Prop']}")
    if pd.notna(row.get("Section")):
        parts.append(f"section: {row['Section']}")
    if pd.notna(row.get("Sub_section")):
        parts.append(f"subsection: {row['Sub_section']}")
    if pd.notna(row.get("Json_pointer")):
        parts.append(f"pointer: {row['Json_pointer']}")
    return " ".join(parts)


def find_crf_int(df: pd.DataFrame) -> int:
    mask = (
        (df["question_label"].astype(str).str.lower() == "crf")
        | (df["Prop"].astype(str).str.lower() == "crf")
    )
    subset = df.loc[mask]
    if not subset.empty:
        val = subset.iloc[0]["Value"]
        try:
            return int(val)
        except Exception:
            pass
    return 1


# -------- COLLECTION --------
def ensure_collection(client: MilvusClient):
    if client.has_collection(COLLECTION):
        return

    schema = client.create_schema(auto_id=False)
    schema.add_field("id", DataType.INT64, is_primary=True)
    schema.add_field("question_label", DataType.VARCHAR, max_length=256)
    schema.add_field("prop", DataType.VARCHAR, max_length=256)
    schema.add_field("value_text", DataType.VARCHAR, max_length=2048)
    schema.add_field("json_pointer", DataType.VARCHAR, max_length=512)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("subsection", DataType.VARCHAR, max_length=256)
    schema.add_field("status", DataType.VARCHAR, max_length=64)
    schema.add_field("dense", DataType.FLOAT_VECTOR, dim=BGE_DIM)
    schema.add_field("sparse", DataType.SPARSE_FLOAT_VECTOR)
    schema.add_field("value_vec", DataType.FLOAT_VECTOR, dim=BGE_DIM)

    idx = client.prepare_index_params()
    idx.add_index("dense", "dense_idx", "AUTOINDEX", "COSINE")
    idx.add_index(
        "sparse",
        "sparse_idx",
        "SPARSE_INVERTED_INDEX",
        "IP",
        params={"inverted_index_algo": "DAAT_MAXSCORE"},
    )
    idx.add_index("value_vec", "value_vec_idx", "AUTOINDEX", "COSINE")

    client.create_collection(COLLECTION, schema=schema, index_params=idx)
    client.load_collection(COLLECTION)


# -------- INGEST ONE FILE --------
def ingest_one_csv(client: MilvusClient, csv_path: str):
    df = pd.read_csv(csv_path, encoding="utf-8-sig").fillna("")
    print(f"\n[+] Reading {os.path.basename(csv_path)} ({len(df)} rows)")

    crf_int = find_crf_int(df)
    print(f"    CRF base: {crf_int}")

    schema_texts = df.apply(build_schema_text, axis=1).tolist()
    tokenized_docs = [txt.lower().split() for txt in schema_texts]
    bm25 = BM25Okapi(tokenized_docs)
    vocab = {tok: i for i, tok in enumerate(sorted({t for doc in tokenized_docs for t in doc}))}

    count = 0
    for idx, (i, row) in enumerate(df.iterrows(), start=1):
        ql = row.get("question_label", "")
        prop = row.get("Prop", "")
        val = row.get("Value", "")
        ptr = row.get("Json_pointer", "")
        sec = row.get("Section", "")
        sub = row.get("Sub_section", "")
        val_norm = normalize_value(val)

        schema_text = schema_texts[idx - 1]
        doc_tokens = schema_text.lower().split()

        schema_dense = ollama_embed(schema_text)

        doc_len = len(doc_tokens)
        avgdl = bm25.avgdl
        k1 = bm25.k1
        b = bm25.b

        tf_in_doc: Dict[str, int] = {}
        for tok in doc_tokens:
            tf_in_doc[tok] = tf_in_doc.get(tok, 0) + 1

        sparse_vec: Dict[int, float] = {}
        for tok, tf in tf_in_doc.items():
            if tok not in bm25.idf:
                continue
            idf = bm25.idf[tok]
            score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
            term_id = vocab[tok]
            sparse_vec[term_id] = float(score)

        if val and not is_json_like(val) and len(val) < 120:
            value_dense = ollama_embed(val)
        else:
            value_dense = [0.0] * BGE_DIM

        pk_int = int(f"{crf_int}{idx}")

        one_row = {
            "id": pk_int,
            "question_label": ql,
            "prop": prop,
            "value_text": val_norm,
            "json_pointer": ptr,
            "section": sec,
            "subsection": sub,
            "status": DEFAULT_STATUS,
            "dense": schema_dense,
            "sparse": sparse_vec,
            "value_vec": value_dense,
        }

        client.insert(COLLECTION, [one_row])
        count += 1

        if count % 100 == 0:
            client.flush(COLLECTION)
            print(f"    Inserted {count} rows...")

    client.flush(COLLECTION)
    print(f"[OK] {os.path.basename(csv_path)} → {count} rows ingested (CRF={crf_int})")

    return vocab, bm25


# -------- MAIN --------
def ingest_directory(client: MilvusClient, dir_path: str):
    ensure_collection(client)
    csv_files = sorted(glob(os.path.join(dir_path, "*.csv")))
    if not csv_files:
        print(f"[!] No CSV files found in {dir_path}")
        return

    print(f"[i] Found {len(csv_files)} CSV files in {dir_path}")

    last_vocab, last_bm25 = None, None
    for csv_file in csv_files:
        vocab, bm25 = ingest_one_csv(client, csv_file)
        last_vocab, last_bm25 = vocab, bm25

    # Save last BM25 model (can be shared for similar schema)
    if last_vocab and last_bm25:
        bm25_state = {
            "k1": last_bm25.k1,
            "b": last_bm25.b,
            "avgdl": last_bm25.avgdl,
            "idf": last_bm25.idf,
            "vocab": last_vocab,
        }
        with open(BM25_MODEL_FILE, "w", encoding="utf-8") as f:
            json.dump(bm25_state, f, ensure_ascii=False, indent=2)
        print(f"[OK] Saved BM25 model to {BM25_MODEL_FILE}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dir", required=True, help="Directory containing CSV files")
    args = ap.parse_args()

    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)
    ingest_directory(client, args.dir)


if __name__ == "__main__":
    main()












#!/usr/bin/env python3
"""
retrieve_milvus_v3_ollama_bm25_rrf.py

Hybrid retrieval (dense + BM25 sparse) from Milvus, matching:
- collection: forms_hybrid_v3
- schema: INT64 PK, question_label, prop, value_text, json_pointer, section, subsection, status,
          dense (1024), sparse, value_vec (1024)
- query encoder: Ollama bge-m3
- sparse leg: BM25, loaded from bm25_model.json
- ranker: RRF (as requested)

Usage:
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address city"
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address" --section "Address" --value "hyderabad"
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "gstin" --status "active" --topk 20
    python retrieve_milvus_v3_ollama_bm25_rrf.py --query "address hyderabad" --value-fuzzy "hyderabad"

Notes:
- If bm25_model.json is missing, we fall back to dense-only.
- If you want fuzzy on value, pass --value-fuzzy; otherwise we just filter exact on value_text.
"""

import os
import json
import argparse
import requests
from typing import Dict, Any, List, Optional

from pymilvus import (
    MilvusClient,
    AnnSearchRequest,
    RRFRanker,
)

# -------- CONFIG --------
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
MILVUS_TOKEN = os.environ.get("MILVUS_TOKEN", "root:Milvus")
COLLECTION = os.environ.get("MILVUS_COLLECTION", "forms_hybrid_v3")

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "bge-m3")

BM25_MODEL_FILE = os.environ.get("BM25_MODEL_FILE", "bm25_model.json")


# -------- OLLAMA --------
def ollama_embed(text: str) -> List[float]:
    payload = {"model": OLLAMA_MODEL, "prompt": text}
    r = requests.post(f"{OLLAMA_URL}/api/embeddings", json=payload, timeout=20)
    r.raise_for_status()
    return r.json()["embedding"]


# -------- BM25 LOAD / QUERY → SPARSE VEC --------
def load_bm25_model(path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def bm25_query_to_sparse(query: str, bm25_state: Dict[str, Any]) -> Dict[int, float]:
    """
    Convert a text query into the same sparse format we used when ingesting.

    We approximate BM25 scoring for the query terms using the stored idf, k1, b, avgdl.
    Query is treated as a short doc.
    """
    tokens = query.lower().split()
    if not tokens:
        return {}

    k1 = bm25_state["k1"]
    b = bm25_state["b"]
    avgdl = bm25_state["avgdl"]
    idf_map = bm25_state["idf"]
    vocab = bm25_state["vocab"]

    # query treated as doc
    doc_len = len(tokens)
    tf_in_q = {}
    for t in tokens:
        tf_in_q[t] = tf_in_q.get(t, 0) + 1

    sparse: Dict[int, float] = {}
    for tok, tf in tf_in_q.items():
        if tok not in idf_map:
            continue
        idf = idf_map[tok]
        score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))
        if tok in vocab:
            term_id = vocab[tok]
            sparse[term_id] = float(score)

    return sparse


# -------- FILTER BUILDER --------
def build_expr(section: str, subsection: str, status: str, value_text: str) -> str:
    clauses = []
    if section:
        clauses.append(f'section == "{section}"')
    if subsection:
        clauses.append(f'subsection == "{subsection}"')
    if status:
        clauses.append(f'status == "{status}"')
    if value_text:
        # we normalized to lowercase at ingest
        vt = value_text.strip().lower()
        clauses.append(f'value_text == "{vt}"')
    return " and ".join(clauses)


# -------- MAIN RETRIEVE --------
def retrieve(
    query: str,
    section: str = "",
    subsection: str = "",
    status: str = "",
    value_text: str = "",
    value_fuzzy: str = "",
    topk: int = 20,
):
    client = MilvusClient(uri=MILVUS_URI, token=MILVUS_TOKEN)

    # embed query for dense leg
    q_dense = ollama_embed(query)

    # try to load BM25 model
    bm25_state = load_bm25_model(BM25_MODEL_FILE)
    q_sparse = None
    if bm25_state is not None:
        q_sparse = bm25_query_to_sparse(query, bm25_state)

    expr = build_expr(section, subsection, status, value_text)

    reqs: List[AnnSearchRequest] = []

    # 1) DENSE leg
    req_dense = AnnSearchRequest(
        data=[q_dense],
        anns_field="dense",
        limit=topk,
        param={"metric_type": "COSINE", "params": {}},
        expr=expr if expr else None,
    )
    reqs.append(req_dense)

    # 2) SPARSE (BM25) leg — only if we have the model
    if q_sparse:
        req_sparse = AnnSearchRequest(
            data=[q_sparse],
            anns_field="sparse",
            limit=topk,
            param={
                "metric_type": "IP",
                "params": {
                    # this is optional; trims tiny weights
                    "drop_ratio_search": 0.2
                },
            },
            expr=expr if expr else None,
        )
        reqs.append(req_sparse)

    # 3) OPTIONAL: value fuzzy leg (search in value_vec)
    if value_fuzzy:
        q_val_dense = ollama_embed(value_fuzzy)
        req_val = AnnSearchRequest(
            data=[q_val_dense],
            anns_field="value_vec",
            limit=topk,
            param={"metric_type": "COSINE", "params": {}},
            # we probably still want section/status filter here:
            expr=build_expr(section, subsection, status, ""),  # no exact value filter
        )
        reqs.append(req_val)

    # RRF: robust across unequal legs
    ranker = RRFRanker()  # default k=60; can tune if needed

    res = client.hybrid_search(
        collection_name=COLLECTION,
        reqs=reqs,
        ranker=ranker,
        limit=topk,
        output_fields=[
            "question_label",
            "prop",
            "value_text",
            "json_pointer",
            "section",
            "subsection",
            "status",
        ],
    )

    # print results
    print(f"\n=== Results for query: {query!r} ===")
    for hits in res:  # we sent one query, so this is 1 list
        for h in hits:
            print("-" * 60)
            print(f"id:            {h.id}")
            print(f"distance:      {h.distance}")
            print(f"label:         {h.get('question_label')}")
            print(f"prop:          {h.get('prop')}")
            print(f"value_text:    {h.get('value_text')}")
            print(f"pointer:       {h.get('json_pointer')}")
            print(f"section:       {h.get('section')}  |  subsection: {h.get('subsection')}")
            print(f"status:        {h.get('status')}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--query", required=True, help="Natural language query, e.g. 'address city'")
    ap.add_argument("--section", default="", help="Filter by section")
    ap.add_argument("--subsection", default="", help="Filter by subsection")
    ap.add_argument("--status", default="", help="Filter by status")
    ap.add_argument(
        "--value",
        default="",
        help="Exact value_text filter (we stored values lowercased). e.g. --value hyderabad",
    )
    ap.add_argument(
        "--value-fuzzy",
        default="",
        help="If provided, we also search value_vec with this text (fuzzy match on values).",
    )
    ap.add_argument("--topk", type=int, default=20, help="How many results to return")
    args = ap.parse_args()

    retrieve(
        query=args.query,
        section=args.section,
        subsection=args.subsection,
        status=args.status,
        value_text=args.value,
        value_fuzzy=args.value_fuzzy,
        topk=args.topk,
    )


if __name__ == "__main__":
    main()
