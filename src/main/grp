# main.py
from __future__ import annotations
from typing import TypedDict, List
from fastapi import FastAPI
from pydantic import BaseModel

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from langchain_ollama import ChatOllama
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage

from chroma_tools import query_chroma

# ---- State
class ChatState(TypedDict):
    messages: List[BaseMessage]
    context: str  # concatenated passages from Chroma

SYSTEM = SystemMessage(content="You are a concise assistant. Use the provided context. If unsure, say so.")

llm = ChatOllama(model="llama3.2")

# ---- Node 1: ALWAYS run Chroma (your method only)
def retrieve(state: ChatState) -> ChatState:
    last_user = next((m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)
    q = last_user.content if last_user else ""
    hits = query_chroma(q, k=5)
    context = "\n\n".join(f"[{i+1}] {doc} (dist={dist:.3f})" for i, (doc, dist) in enumerate(hits))
    return {"messages": state["messages"], "context": context}

# ---- Node 2: Ask LLM using only that context
def answer(state: ChatState) -> ChatState:
    msgs = [
        SYSTEM,
        SystemMessage(content=f"Context:\n{state['context']}"),
        *state["messages"]
    ]
    ai: AIMessage = llm.invoke(msgs)
    return {"messages": state["messages"] + [ai], "context": state["context"]}

# ---- Build graph: START -> retrieve -> answer -> END
graph = StateGraph(ChatState)
graph.add_node("retrieve", retrieve)
graph.add_node("answer", answer)
graph.add_edge(START, "retrieve")
graph.add_edge("retrieve", "answer")
graph.add_edge("answer", END)

app_graph = graph.compile(checkpointer=MemorySaver())

# ---- FastAPI
api = FastAPI()

class ChatRequest(BaseModel):
    session_id: str
    user_input: str

class ChatResponse(BaseModel):
    answer: str

@api.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    state = {"messages": [HumanMessage(content=req.user_input)], "context": ""}
    out: ChatState = app_graph.invoke(
        state, config={"configurable": {"thread_id": req.session_id}}
    )
    return ChatResponse(answer=out["messages"][-1].content)

# Run: uvicorn main:api --reload





from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_ollama import ChatOllama

llm = ChatOllama(model="llama3.2")
SYSTEM = SystemMessage(content="Be concise. Use Context when relevant.")

def rewrite_query(state: MessagesState) -> dict:
    recent = state["messages"][-8:]  # <- history is coming from LangGraph state
    prompt = [
        SystemMessage(content=(
            "Rewrite the user's latest message into a standalone search query. "
            "Resolve references using recent conversation. Return only the query."
        )),
        *recent
    ]
    rewritten = llm.invoke(prompt).content.strip()
    if not rewritten:  # fallback to last human
        for m in reversed(state["messages"]):
            if m.type == "human":
                rewritten = m.content
                break
    return {"retriever_query": rewritten}

def retrieve(state: MessagesState) -> dict:
    q = state.get("retriever_query", "")
    docs = chroma_search(q)      # your function; returns List[str]
    return {"context": "\n\n".join(docs)}

def answer(state: MessagesState) -> MessagesState:
    msgs = [SYSTEM, SystemMessage(content=f"Context:\n{state.get('context','')}"),
            *state["messages"]]
    ai: AIMessage = llm.invoke(msgs)
    return {"messages": [ai]}    # append only the new AI message

g = StateGraph(MessagesState)
g.add_node("rewrite_query", rewrite_query)
g.add_node("retrieve", retrieve)
g.add_node("answer", answer)
g.add_edge(START, "rewrite_query")
g.add_edge("rewrite_query", "retrieve")
g.add_edge("retrieve", "answer")
g.add_edge("answer", END)

app_graph = g.compile(checkpointer=SqliteSaver("chat_memory.sqlite"))